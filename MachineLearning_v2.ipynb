{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 132) (125546, 131)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat((train_data,test_data), ignore_index = True, axis = 0)\n",
    "del( train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Categorical features:', 116)\n",
      "('Numerica features:', 14)\n",
      "ID: id, target: loss\n"
     ]
    }
   ],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')\n",
    "\n",
    "print (\"Categorical features:\", len(cat_cols))\n",
    "print ( \"Numerica features:\", len(num_cols))\n",
    "print ( \"ID: %s, target: %s\" %( id_col, target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation (cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#comb_features = [['cat103','cat111'],['cat72','cat103'],['cat80','cat81'],['cat73','cat1'],['cat6','cat103'],['cat80','cat79'],['cat111','cat2'],['cat50','cat111'],['cat9','cat90'],['cat76','cat111'],['cat111','cat13'],['cat79','cat12'],['cat103','cat11'],['cat103','cat4'],['cat111','cat87'],['cat111','cat38'],['cat111','cat36'],['cat25','cat2'],['cat103','cat23'],['cat103','cat10'],['cat111','cat5'],['cat80','cat57'],['cat24','cat103'],['cat7','cat87'],['cat80','cat3'],['cat73','cat40'],['cat85','cat79'],['cat16','cat57'],['cat24','cat28'],['cat46','cat79'],['cat87','cat89'],['cat60','cat73'],['cat9','cat22'],['cat6','cat14'],['cat9','cat47'],['cat9','cat70'],['cat34','cat57'],['cat55','cat57']]\n",
    "# 101_comb_features, ['cat72_cat103', 'cat80_cat79', 'cat81_cat90'] may increase the mae\n",
    "comb_features = [['cat103','cat111'], ['cat72','cat103'], ['cat80','cat81'], ['cat73','cat1'], ['cat72','cat111'], \\\n",
    "                ['cat6','cat103'], ['cat6','cat111'], ['cat80','cat79'], ['cat1','cat111'], ['cat79','cat103'], \\\n",
    "                ['cat111','cat2'], ['cat79','cat111'], ['cat50','cat111'], ['cat73','cat81'], ['cat72','cat2'], \\\n",
    "                ['cat50','cat103'], ['cat1','cat81'], ['cat1','cat103'], ['cat73','cat103'], ['cat6','cat2'], \\\n",
    "                ['cat80','cat111'], ['cat80','cat103'], ['cat1','cat72'], ['cat1','cat79'], ['cat103','cat2'], \\\n",
    "                ['cat81','cat103'], ['cat9','cat90'], ['cat73','cat111'], ['cat111','cat9'], ['cat76','cat111'], \\\n",
    "                ['cat76','cat103'], ['cat72','cat9'], ['cat6','cat79'], ['cat103','cat9'], ['cat79','cat81'], \\\n",
    "                ['cat111','cat13'], ['cat79','cat72'], ['cat80','cat72'], ['cat1','cat50'], ['cat72','cat81'], \\\n",
    "                ['cat79','cat12'], ['cat81','cat90'], ['cat81','cat111'], ['cat80','cat1'], ['cat103','cat11'], \\\n",
    "                ['cat73','cat79'], ['cat6','cat72'], ['cat103','cat4'], ['cat73','cat50'], ['cat103','cat12'], \\\n",
    "                ['cat111','cat87'], ['cat6','cat87'], ['cat111','cat12'], ['cat111','cat38'], ['cat6','cat9'], \\\n",
    "                ['cat111','cat36'], ['cat50','cat2'], ['cat103','cat87'], ['cat25','cat2'], ['cat103','cat23'], \\\n",
    "                ['cat73','cat72'], ['cat72','cat36'], ['cat80','cat2'], ['cat103','cat10'], ['cat50','cat72'], \\\n",
    "                ['cat72','cat87'], ['cat1','cat9'], ['cat79','cat2'], ['cat80','cat87'], ['cat111','cat11'], \\\n",
    "                ['cat76','cat79'], ['cat72','cat10'], ['cat80','cat73'], ['cat72','cat12'], ['cat79','cat87'], \\\n",
    "                ['cat6','cat73'], ['cat72','cat23'], ['cat12','cat38'], ['cat36','cat23'], ['cat6','cat36'], \\\n",
    "                ['cat50','cat9'], ['cat111','cat10'], ['cat111','cat5'], ['cat36','cat9'], ['cat36','cat2'], \\\n",
    "                ['cat50','cat87'], ['cat80','cat57'], ['cat6','cat1'], ['cat73','cat9'], ['cat1','cat2'], \\\n",
    "                ['cat23','cat2'], ['cat11','cat87'], ['cat38','cat2'], ['cat76','cat2'], ['cat72','cat25'], \\\n",
    "                ['cat24','cat103'], ['cat6','cat12'], ['cat6','cat80'], ['cat72','cat11'], ['cat103','cat13'], \\\n",
    "                ['cat79','cat9']]\n",
    "\n",
    "cat_add = []\n",
    "for comb in comb_features:\n",
    "    full_data[comb[0] + \"_\" + comb[1]] = full_data[comb[0]] + full_data[comb[1]]\n",
    "    cat_add.append(comb[0] + \"_\" + comb[1])\n",
    "cat_cols = cat_cols + cat_add\n",
    "\n",
    "# Original combination features: https://www.kaggle.com/misfyre/allstate-claims-severity/encoding-feature-comb-modkzs-1108-72665/discussion\n",
    "#import itertools\n",
    "#comb_list = ['cat80', 'cat87', 'cat57', 'cat12', 'cat79', 'cat10', 'cat7', 'cat89', 'cat2', 'cat72', 'cat81', 'cat11', \\\n",
    "#             'cat1', 'cat13', 'cat9', 'cat3', 'cat16', 'cat90', 'cat23', 'cat36', 'cat73', 'cat103', 'cat40', 'cat28', \\\n",
    "#             'cat111', 'cat6', 'cat76', 'cat50', 'cat5', 'cat4', 'cat14', 'cat38', 'cat24', 'cat82', 'cat25']\n",
    "# Added 'cat37','cat27','cat53','cat44'\n",
    "#comb_list = ['cat37','cat27','cat53','cat44'] + comb_list\n",
    "#cat_add = []\n",
    "#for comb in itertools.combinations(comb_list, 2):\n",
    "#    full_data[comb[0] + \"_\" + comb[1]] = full_data [comb[0]] + full_data [ comb[1]]\n",
    "#    cat_add.append(comb[0] + \"_\" + comb[1])\n",
    "cat_cols = cat_cols + cat_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features\n",
    "### 1. Label Encoding (Factorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "for cat_col in cat_cols:\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One Hot Encoding (get dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave-one-out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# loo_cols =[]\n",
    "# for col in cat_cols:\n",
    "#     print (\"Leave-One-Out Encoding  %s\" % (col))\n",
    "#     print (\"Leave-one-out encoding column %s for %s......\" % (col, target_col))\n",
    "#     aggr=full_data.groupby(col)[target_col].agg([np.mean]).join(full_data[:train_size].groupby(col)[target_col].agg([np.sum,np.size]),how='left')        \n",
    "#     meanTagetAggr = np.mean(aggr['mean'].values)\n",
    "#     aggr=full_data.join(aggr,how='left', on=col)[list(aggr.columns)+[target_col]]\n",
    "#     loo_col = 'MEAN_BY_'+col+'_'+target_col\n",
    "#     full_data[loo_col] = \\\n",
    "#     aggr.apply(lambda row: row['mean'] if math.isnan(row[target_col]) \n",
    "#                                                        else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(0.95, 1.05) , axis=1)\n",
    "#     loo_cols.append(loo_col)\n",
    "#     print (\"New feature %s created.\" % (loo_col))\n",
    "# print ('Leave-one-out enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric features\n",
    "### Calculate skewness of each numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont2           -0.310939\n",
      "cont14_cat113   -0.242597\n",
      "cont3           -0.010002\n",
      "cont14           0.248672\n",
      "cont14_cat112    0.250998\n",
      "cont11           0.280819\n",
      "cont12           0.291990\n",
      "cont10           0.354998\n",
      "cont13           0.380739\n",
      "cont4            0.416093\n",
      "cont6            0.461211\n",
      "cont1            0.516420\n",
      "cont8            0.676629\n",
      "cont5            0.681617\n",
      "cont7            0.826046\n",
      "cont14_cat100    0.893204\n",
      "cont9            1.072420\n",
      "cont14_cat116    1.409188\n",
      "cont14_cat110    1.672612\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew, boxcox\n",
    "skewed_cols = full_data[:train_size][num_cols].apply(lambda x: skew(x.dropna()))\n",
    "print (skewed_cols.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply box-cox transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewed_cols = skewed_cols[skewed_cols > 0.24].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Standard Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "SSL = preprocessing.StandardScaler()\n",
    "for num_col in num_cols:\n",
    "    full_data[num_col] = SSL.fit_transform(full_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat100</th>\n",
       "      <th>cat101</th>\n",
       "      <th>cat102</th>\n",
       "      <th>cat103</th>\n",
       "      <th>cat104</th>\n",
       "      <th>cat105</th>\n",
       "      <th>cat106</th>\n",
       "      <th>cat107</th>\n",
       "      <th>...</th>\n",
       "      <th>cat6_cat12</th>\n",
       "      <th>cat6_cat80</th>\n",
       "      <th>cat72_cat11</th>\n",
       "      <th>cat103_cat13</th>\n",
       "      <th>cat79_cat9</th>\n",
       "      <th>cont14_cat100</th>\n",
       "      <th>cont14_cat112</th>\n",
       "      <th>cont14_cat113</th>\n",
       "      <th>cont14_cat110</th>\n",
       "      <th>cont14_cat116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.504525</td>\n",
       "      <td>0.718677</td>\n",
       "      <td>0.511825</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.529868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.499385</td>\n",
       "      <td>0.310124</td>\n",
       "      <td>0.512084</td>\n",
       "      <td>0.489072</td>\n",
       "      <td>0.462916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.499385</td>\n",
       "      <td>0.767099</td>\n",
       "      <td>0.442002</td>\n",
       "      <td>0.441747</td>\n",
       "      <td>0.471563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>0.599762</td>\n",
       "      <td>0.489333</td>\n",
       "      <td>0.488853</td>\n",
       "      <td>0.479936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.496052</td>\n",
       "      <td>0.442130</td>\n",
       "      <td>0.512084</td>\n",
       "      <td>0.494981</td>\n",
       "      <td>0.492886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat10  cat100  cat101  cat102  cat103  cat104  cat105  cat106  \\\n",
       "0     0      0       1       6       0       0       8       4       6   \n",
       "1     0      1      11       5       0       0       4       4       8   \n",
       "2     0      1      11      14       0       1       4       5       7   \n",
       "3     1      0       8       3       0       0       4       4       8   \n",
       "4     0      1       5       9       0       0       3       4      10   \n",
       "\n",
       "   cat107      ...        cat6_cat12  cat6_cat80  cat72_cat11  cat103_cat13  \\\n",
       "0       9      ...                 0           3            1             0   \n",
       "1      10      ...                 0           3            0             0   \n",
       "2       5      ...                 1           1            1             3   \n",
       "3      10      ...                 0           3            0             0   \n",
       "4       6      ...                 1           1            2             0   \n",
       "\n",
       "   cat79_cat9  cont14_cat100  cont14_cat112  cont14_cat113  cont14_cat110  \\\n",
       "0           3       0.504525       0.718677       0.511825       0.513514   \n",
       "1           3       0.499385       0.310124       0.512084       0.489072   \n",
       "2           3       0.499385       0.767099       0.442002       0.441747   \n",
       "3           3       0.485629       0.599762       0.489333       0.488853   \n",
       "4           7       0.496052       0.442130       0.512084       0.494981   \n",
       "\n",
       "   cont14_cat116  \n",
       "0       0.529868  \n",
       "1       0.462916  \n",
       "2       0.471563  \n",
       "3       0.479936  \n",
       "4       0.492886  \n",
       "\n",
       "[5 rows x 238 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/tilii7/allstate-claims-severity/feature-importance-from-linear-and-tree-models/code\n",
    "# Generate interaction between cont7/cont14 and 5 most important cats from tree-based and 6 most important cats from LR\n",
    "#target_keys = ['cont1', 'cont7', 'cont14']\n",
    "#aList = ['cat100', 'cat112', 'cat113', 'cat110', 'cat116', 'cat57', 'cat53', 'cat77', 'cat12', 'cat44', 'cat90']\n",
    "target_keys = ['cont14']\n",
    "aList = ['cat100', 'cat112', 'cat113', 'cat110', 'cat116']\n",
    "num_add = []\n",
    "for target_key in target_keys:\n",
    "    for key in aList:\n",
    "        gb = full_data[[target_key,key]].groupby(key).agg({target_key: 'mean'})\n",
    "        full_data[target_key+'_'+key] = full_data[key].map(lambda x: gb.loc[x, target_key])\n",
    "        num_add.append(target_key+'_'+key)\n",
    "#print len(num_add)\n",
    "\n",
    "# 20 num_cat combs decrease mae\n",
    "#num_add = []\n",
    "#comb_features = [['cont1','cat100'], ['cont1','cat112'], ['cont1','cat116'], ['cont1','cat57'], ['cont1','cat12'], ['cont1','cat44'], ['cont7','cat100'], ['cont7','cat110'], ['cont7','cat116'], ['cont7','cat57'], ['cont7','cat12'], ['cont7','cat44'], ['cont7','cat90'], ['cont14','cat100'], ['cont14','cat112'], ['cont14','cat116'], ['cont14','cat57'], ['cont14','cat53'], ['cont14','cat12'], ['cont14','cat90']]\n",
    "#for comb in comb_features:\n",
    "#    gb = full_data[[comb[0],comb[1]]].groupby(comb[1]).agg({comb[0]: 'mean'})\n",
    "#    full_data[comb[0]+'_'+comb[1]] = full_data[comb[1]].map(lambda x: gb.ix[x, comb[0]])\n",
    "#    num_add.append(comb[0]+'_'+comb[1])\n",
    "num_cols = num_cols + num_add\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n",
      "('key: ', 'cat103_cat111', 'Best round: ', 195, 'MAE: ', 1148.5096622134056)\n",
      "('key: ', 'cat72_cat103', 'Best round: ', 223, 'MAE: ', 1151.726495506688)\n",
      "('key: ', 'cat80_cat81', 'Best round: ', 278, 'MAE: ', 1147.1898693139321)\n",
      "('key: ', 'cat73_cat1', 'Best round: ', 253, 'MAE: ', 1150.869728739375)\n",
      "('key: ', 'cat72_cat111', 'Best round: ', 278, 'MAE: ', 1149.3486489581198)\n",
      "('key: ', 'cat6_cat103', 'Best round: ', 242, 'MAE: ', 1151.4513942030712)\n",
      "('key: ', 'cat6_cat111', 'Best round: ', 207, 'MAE: ', 1149.8048013618293)\n",
      "('key: ', 'cat80_cat79', 'Best round: ', 183, 'MAE: ', 1152.2043645298631)\n",
      "('key: ', 'cat1_cat111', 'Best round: ', 222, 'MAE: ', 1151.643939564628)\n",
      "('key: ', 'cat79_cat103', 'Best round: ', 224, 'MAE: ', 1149.4073680516533)\n",
      "('key: ', 'cat111_cat2', 'Best round: ', 264, 'MAE: ', 1148.7091043346877)\n",
      "('key: ', 'cat79_cat111', 'Best round: ', 243, 'MAE: ', 1148.7892424302429)\n",
      "('key: ', 'cat50_cat111', 'Best round: ', 280, 'MAE: ', 1148.9137270332938)\n",
      "('key: ', 'cat73_cat81', 'Best round: ', 235, 'MAE: ', 1147.9660299452473)\n",
      "('key: ', 'cat72_cat2', 'Best round: ', 198, 'MAE: ', 1150.2131490574966)\n",
      "('key: ', 'cat50_cat103', 'Best round: ', 250, 'MAE: ', 1150.5933615923302)\n",
      "('key: ', 'cat1_cat81', 'Best round: ', 268, 'MAE: ', 1150.8301381263084)\n",
      "('key: ', 'cat1_cat103', 'Best round: ', 194, 'MAE: ', 1149.6529812790193)\n",
      "('key: ', 'cat73_cat103', 'Best round: ', 242, 'MAE: ', 1148.4473984839137)\n",
      "('key: ', 'cat6_cat2', 'Best round: ', 189, 'MAE: ', 1149.7496527548251)\n",
      "('key: ', 'cat80_cat111', 'Best round: ', 235, 'MAE: ', 1149.5445213940434)\n",
      "('key: ', 'cat80_cat103', 'Best round: ', 221, 'MAE: ', 1149.7052700357328)\n",
      "('key: ', 'cat1_cat72', 'Best round: ', 206, 'MAE: ', 1150.4268977288368)\n",
      "('key: ', 'cat1_cat79', 'Best round: ', 231, 'MAE: ', 1149.5744121852013)\n",
      "('key: ', 'cat103_cat2', 'Best round: ', 192, 'MAE: ', 1150.3132933543661)\n",
      "('key: ', 'cat81_cat103', 'Best round: ', 211, 'MAE: ', 1147.3312393990855)\n",
      "('key: ', 'cat9_cat90', 'Best round: ', 214, 'MAE: ', 1148.7126479966989)\n",
      "('key: ', 'cat73_cat111', 'Best round: ', 188, 'MAE: ', 1149.9376974291119)\n",
      "('key: ', 'cat111_cat9', 'Best round: ', 264, 'MAE: ', 1149.9074179535939)\n",
      "('key: ', 'cat76_cat111', 'Best round: ', 268, 'MAE: ', 1149.6716877510939)\n",
      "('key: ', 'cat76_cat103', 'Best round: ', 295, 'MAE: ', 1148.7711682083568)\n",
      "('key: ', 'cat72_cat9', 'Best round: ', 193, 'MAE: ', 1150.0464274481769)\n",
      "('key: ', 'cat6_cat79', 'Best round: ', 274, 'MAE: ', 1147.604495459937)\n",
      "('key: ', 'cat103_cat9', 'Best round: ', 209, 'MAE: ', 1148.8903722606553)\n",
      "('key: ', 'cat79_cat81', 'Best round: ', 238, 'MAE: ', 1149.1246198679044)\n",
      "('key: ', 'cat111_cat13', 'Best round: ', 246, 'MAE: ', 1147.9858598838298)\n",
      "('key: ', 'cat79_cat72', 'Best round: ', 272, 'MAE: ', 1149.6184216784713)\n",
      "('key: ', 'cat80_cat72', 'Best round: ', 233, 'MAE: ', 1150.5570106654648)\n",
      "('key: ', 'cat1_cat50', 'Best round: ', 295, 'MAE: ', 1149.4744107946347)\n",
      "('key: ', 'cat72_cat81', 'Best round: ', 260, 'MAE: ', 1149.7280252059209)\n",
      "('key: ', 'cat79_cat12', 'Best round: ', 222, 'MAE: ', 1148.4164794100761)\n",
      "('key: ', 'cat81_cat90', 'Best round: ', 186, 'MAE: ', 1152.8479296549963)\n",
      "('key: ', 'cat81_cat111', 'Best round: ', 262, 'MAE: ', 1149.6561166241254)\n",
      "('key: ', 'cat80_cat1', 'Best round: ', 279, 'MAE: ', 1150.8964991234457)\n",
      "('key: ', 'cat103_cat11', 'Best round: ', 299, 'MAE: ', 1149.4693218230491)\n",
      "('key: ', 'cat73_cat79', 'Best round: ', 349, 'MAE: ', 1148.9885012588929)\n",
      "('key: ', 'cat6_cat72', 'Best round: ', 208, 'MAE: ', 1149.7407400001223)\n",
      "('key: ', 'cat103_cat4', 'Best round: ', 259, 'MAE: ', 1147.5972914446297)\n",
      "('key: ', 'cat73_cat50', 'Best round: ', 203, 'MAE: ', 1147.7375389880774)\n",
      "('key: ', 'cat103_cat12', 'Best round: ', 228, 'MAE: ', 1149.1736315260114)\n",
      "('key: ', 'cat111_cat87', 'Best round: ', 231, 'MAE: ', 1148.7394442655107)\n",
      "('key: ', 'cat6_cat87', 'Best round: ', 254, 'MAE: ', 1150.0431700878542)\n",
      "('key: ', 'cat111_cat12', 'Best round: ', 290, 'MAE: ', 1148.9231414528842)\n",
      "('key: ', 'cat111_cat38', 'Best round: ', 225, 'MAE: ', 1147.389095777741)\n",
      "('key: ', 'cat6_cat9', 'Best round: ', 266, 'MAE: ', 1151.1484406711891)\n",
      "('key: ', 'cat111_cat36', 'Best round: ', 239, 'MAE: ', 1151.1673493191861)\n",
      "('key: ', 'cat50_cat2', 'Best round: ', 253, 'MAE: ', 1150.2201528688397)\n",
      "('key: ', 'cat103_cat87', 'Best round: ', 257, 'MAE: ', 1150.4538983564755)\n",
      "('key: ', 'cat25_cat2', 'Best round: ', 254, 'MAE: ', 1149.9884546751091)\n",
      "('key: ', 'cat103_cat23', 'Best round: ', 238, 'MAE: ', 1148.3096103038388)\n",
      "('key: ', 'cat73_cat72', 'Best round: ', 254, 'MAE: ', 1150.3419899079968)\n",
      "('key: ', 'cat72_cat36', 'Best round: ', 214, 'MAE: ', 1150.8503610116629)\n",
      "('key: ', 'cat80_cat2', 'Best round: ', 267, 'MAE: ', 1149.413284588557)\n",
      "('key: ', 'cat103_cat10', 'Best round: ', 306, 'MAE: ', 1146.9721938720804)\n",
      "('key: ', 'cat50_cat72', 'Best round: ', 198, 'MAE: ', 1150.4138049615055)\n",
      "('key: ', 'cat72_cat87', 'Best round: ', 258, 'MAE: ', 1150.3322018415286)\n",
      "('key: ', 'cat1_cat9', 'Best round: ', 256, 'MAE: ', 1149.5062971845143)\n",
      "('key: ', 'cat79_cat2', 'Best round: ', 224, 'MAE: ', 1150.0726391903222)\n",
      "('key: ', 'cat80_cat87', 'Best round: ', 294, 'MAE: ', 1147.8400354013652)\n",
      "('key: ', 'cat111_cat11', 'Best round: ', 266, 'MAE: ', 1148.173820986995)\n",
      "('key: ', 'cat76_cat79', 'Best round: ', 255, 'MAE: ', 1149.3431318594801)\n",
      "('key: ', 'cat72_cat10', 'Best round: ', 251, 'MAE: ', 1150.6722302582991)\n",
      "('key: ', 'cat80_cat73', 'Best round: ', 283, 'MAE: ', 1147.0669811599168)\n",
      "('key: ', 'cat72_cat12', 'Best round: ', 254, 'MAE: ', 1149.948646023933)\n",
      "('key: ', 'cat79_cat87', 'Best round: ', 254, 'MAE: ', 1149.86818177802)\n",
      "('key: ', 'cat6_cat73', 'Best round: ', 218, 'MAE: ', 1149.0881779702734)\n",
      "('key: ', 'cat72_cat23', 'Best round: ', 240, 'MAE: ', 1150.3274610281321)\n",
      "('key: ', 'cat12_cat38', 'Best round: ', 294, 'MAE: ', 1148.6723473199263)\n",
      "('key: ', 'cat36_cat23', 'Best round: ', 288, 'MAE: ', 1148.306037920391)\n",
      "('key: ', 'cat6_cat36', 'Best round: ', 243, 'MAE: ', 1149.8877254239246)\n",
      "('key: ', 'cat50_cat9', 'Best round: ', 287, 'MAE: ', 1150.6684648682647)\n",
      "('key: ', 'cat111_cat10', 'Best round: ', 216, 'MAE: ', 1149.8926828131205)\n",
      "('key: ', 'cat111_cat5', 'Best round: ', 270, 'MAE: ', 1150.4241246840991)\n",
      "('key: ', 'cat36_cat9', 'Best round: ', 274, 'MAE: ', 1148.6807929879474)\n",
      "('key: ', 'cat36_cat2', 'Best round: ', 294, 'MAE: ', 1148.4531606679868)\n",
      "('key: ', 'cat50_cat87', 'Best round: ', 185, 'MAE: ', 1150.4151000364354)\n",
      "('key: ', 'cat80_cat57', 'Best round: ', 212, 'MAE: ', 1148.2399567148946)\n",
      "('key: ', 'cat6_cat1', 'Best round: ', 290, 'MAE: ', 1149.6523984029905)\n",
      "('key: ', 'cat73_cat9', 'Best round: ', 210, 'MAE: ', 1148.3618345076782)\n",
      "('key: ', 'cat1_cat2', 'Best round: ', 263, 'MAE: ', 1149.4774078342596)\n",
      "('key: ', 'cat23_cat2', 'Best round: ', 244, 'MAE: ', 1150.4236825173755)\n",
      "('key: ', 'cat11_cat87', 'Best round: ', 211, 'MAE: ', 1150.8541963589055)\n",
      "('key: ', 'cat38_cat2', 'Best round: ', 236, 'MAE: ', 1149.4441502395489)\n",
      "('key: ', 'cat76_cat2', 'Best round: ', 216, 'MAE: ', 1148.2871539710666)\n",
      "('key: ', 'cat72_cat25', 'Best round: ', 219, 'MAE: ', 1150.2224205913381)\n",
      "('key: ', 'cat24_cat103', 'Best round: ', 251, 'MAE: ', 1149.117497946919)\n",
      "('key: ', 'cat6_cat12', 'Best round: ', 198, 'MAE: ', 1149.0512097793071)\n",
      "('key: ', 'cat6_cat80', 'Best round: ', 325, 'MAE: ', 1149.4464511339445)\n",
      "('key: ', 'cat72_cat11', 'Best round: ', 232, 'MAE: ', 1148.5289886950989)\n",
      "('key: ', 'cat103_cat13', 'Best round: ', 285, 'MAE: ', 1148.2619802914598)\n",
      "('key: ', 'cat79_cat9', 'Best round: ', 201, 'MAE: ', 1151.2763723887347)\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering (add interacted features again)\n",
    "from pylightgbm.models import GBMRegressor\n",
    "execpath = '/Users/didle/OtherSoftwares/LightGBM/lightgbm'\n",
    "\n",
    "lift = 200\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "\n",
    "#full_data_sparse = sparse.hstack((full_data_sparse, full_data[num_cols+num_add]), format='csr')\n",
    "#train_x = full_data_sparse[:train_size]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)\n",
    "\n",
    "rgr = GBMRegressor(exec_path=execpath,learning_rate=0.1,metric = 'l1',num_threads = 4,num_iterations=10000,\n",
    "                       early_stopping_round=50,max_bin=483, num_leaves=121, min_data_in_leaf=107, \n",
    "                       feature_fraction=0.195979, bagging_fraction=0.918178, verbose = False)\n",
    "#rgr.fit(X_train, y_train, test_data=[(X_val,y_val)])\n",
    "#y_pred = rgr.predict(X_val)\n",
    "#print(\"Basic, Best round: \", rgr.best_round, \"MAE: \", log_mae(y_val,y_pred))\n",
    "    \n",
    "for col in cat_add:\n",
    "    OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "    full_data_sparse=OHE.fit_transform(full_data[cat_cols+[col]])\n",
    "\n",
    "    full_data_sparse = sparse.hstack((full_data_sparse, full_data[num_cols]), format='csr')\n",
    "    train_x = full_data_sparse[:train_size]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)\n",
    "    \n",
    "    rgr.fit(X_train, y_train, test_data=[(X_val,y_val)])\n",
    "    y_pred = rgr.predict(X_val)\n",
    "    print(\"key: \", col, \"Best round: \", rgr.best_round, \"MAE: \", log_mae(y_val,y_pred))\n",
    "\n",
    "# basic: 1151.7255222311139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 4815)\n"
     ]
    }
   ],
   "source": [
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse, full_data[num_cols]), format='csr')\n",
    "print (full_data_sparse.shape)\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[train_size:].values\n",
    "del full_data, full_data_sparse\n",
    "\n",
    "#train_x = full_data[:train_size]\n",
    "#test_x = full_data[train_size:]\n",
    "#train_x = train_x[num_cols + cat_cols].values\n",
    "#test_x = test_x[num_cols + cat_cols].values\n",
    "#del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grad - is first derivative of a function and hess is a second derivative of your loss function\n",
    "# objective = 'reg:linear': minimize MSE, optimizes for the mean\n",
    "# This problem should minimize MAE, optimizes for the median\n",
    "\n",
    "# The median for log-normal distribution is exp(u) (log(Loss) ~ N(u, var)) \n",
    "# Thus, if you do log transformation on dependent variable and then use MSE to estimate mean of log(Loss) \n",
    "# would essentially equal with estimation of median of Loss.\n",
    "def logregobj(labels, preds):\n",
    "    con = 2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "def xgb_logregobj(preds, dtrain):\n",
    "    con = 2\n",
    "    labels = dtrain.get_label()\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess\n",
    "\n",
    "def fairobj(labels, preds):\n",
    "    fair_constant = 0.7\n",
    "    x = (preds - labels)\n",
    "    den = abs(x) + fair_constant\n",
    "    grad = fair_constant * x / (den)\n",
    "    hess = fair_constant * fair_constant / (den * den)\n",
    "    return grad, hess\n",
    "\n",
    "def xgb_fairobj(preds, dtrain):\n",
    "    fair_constant = 0.7\n",
    "    labels = dtrain.get_label()\n",
    "    x = (preds - labels)\n",
    "    den = abs(x) + fair_constant\n",
    "    grad = fair_constant * x / (den)\n",
    "    hess = fair_constant * fair_constant / (den * den)\n",
    "    return grad, hess\n",
    "\n",
    "# loss function for Cachy function\n",
    "def cauchylobj(labels, preds):\n",
    "    c = 2  #the lower the \"slower/smoother\" the loss is. Cross-Validate.\n",
    "    x = (preds - labels)\n",
    "    grad = x / ((x**2)/(c**2)+1)\n",
    "    hess = -c**2 * (x**2 - c**2) / (x**2+c**2)**2\n",
    "    return grad, hess\n",
    "\n",
    "def xgb_cauchylobj(preds, dtrain):\n",
    "    c = 2  #the lower the \"slower/smoother\" the loss is. Cross-Validate.\n",
    "    labels = dtrain.get_label()\n",
    "    x = (preds - labels)\n",
    "    grad = x / ((x**2)/(c**2)+1)\n",
    "    hess = -c**2 * (x**2 - c**2) / (x**2+c**2)**2\n",
    "    return grad, hess\n",
    "\n",
    "# loss function for ln(cosh(x)) objective, where x is absolute error of normally distributed random variable\n",
    "# convergent slowly\n",
    "#def logcoshobj(preds, dtrain):\n",
    "#    labels = dtrain.get_label()\n",
    "#    grad = np.tanh(preds - labels)\n",
    "#    hess = 1.0 - grad*grad\n",
    "#    return grad, hess\n",
    "\n",
    "# loss function for log(exp(-x) + exp(x)), eqaulivent to the loss function ln(cosh(x))\n",
    "#def logexpexp(preds, dtrain):\n",
    "#    labels = dtrain.get_label()\n",
    "#    x= preds - labels\n",
    "#    grad = (np.exp(2.0*x) - 1) / (np.exp(2.0*x) + 1)\n",
    "#    hess = (4.0*np.exp(2.0*x)) / (np.exp(2.0*x) + 1)**2 \n",
    "#    return grad, hess\n",
    "\n",
    "def log_mae(labels,preds, lift=200):\n",
    "    return mean_absolute_error(np.exp(labels)-lift, np.exp(preds)-lift)\n",
    "\n",
    "def eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
    "\n",
    "def lgbm_eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mannual Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val: ', 0, 'MAE: ', 1146.1576988535892, 'best_round:', 1004)\n",
      "('val: ', 0.003, 'MAE: ', 1146.1576988535892, 'best_round:', 1004)\n",
      "('val: ', 0.01, 'MAE: ', 1146.1576988535892, 'best_round:', 1004)\n",
      "('val: ', 0.03, 'MAE: ', 1146.1576988535892, 'best_round:', 1004)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-5ccf2dfbed41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                        \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                       )\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MAE: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_mae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_round:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    249\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mboost\u001b[0;34m(self, dtrain, grad, hess)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         _check_call(_LIB.XGBoosterBoostOneIter(self.handle, dtrain.handle,\n\u001b[0;32m--> 845\u001b[0;31m                                                \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m                                                \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                                                len(grad)))\n",
      "\u001b[0;32m/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m\"\"\"Convert a python string to c array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mannual: max_depth (5~10), min_child_weight (1+, 5*), colsample_bytree (0.3-0.9), subsample (0.6-1), gamma (0~3, 0.3)\n",
    "learning_rate = 0.1\n",
    "obj = logregobj\n",
    "\n",
    "max_depth = 5\n",
    "min_child_weight = 200\n",
    "colsample_bytree = 1.0\n",
    "subsample = 1.0\n",
    "\n",
    "for val in [0, 0.003, 0.01, 0.03, 0.1]:\n",
    "    rgr = xgb.XGBRegressor( seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                       learning_rate = learning_rate,\n",
    "                       n_estimators = 10000,\n",
    "                       objective = obj,\n",
    "                       nthread = -1,\n",
    "                       silent = True,\n",
    "                       max_depth = max_depth,\n",
    "                       min_child_weight = min_child_weight,\n",
    "                       colsample_bytree = colsample_bytree,\n",
    "                       subsample = subsample,\n",
    "                       gamma = val\n",
    "                      )\n",
    "    rgr.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=eval_mae, early_stopping_rounds=50, verbose = False)\n",
    "    y_pred = rgr.predict(X_val)\n",
    "    print(\"val: \", val, \"MAE: \", log_mae(y_val,y_pred), \"best_round:\", rgr.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "min_child_weight = 200\n",
    "colsample_bytree = 1.0\n",
    "subsample = 1.0\n",
    "gamma = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Automated tuning - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[5476]\ttrain-mae:1104.56+1.89461\ttest-mae:1142.17+6.53569\n",
      "\n",
      "    1 | 110m44s | \u001b[35m-1142.17499\u001b[0m | \u001b[32m            0.9429\u001b[0m | \u001b[32m   0.1939\u001b[0m | \u001b[32m     2.6780\u001b[0m | \u001b[32m          193.1654\u001b[0m | \u001b[32m     0.8772\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1536]\ttrain-mae:1070.11+1.6556\ttest-mae:1138.78+6.26471\n",
      "\n",
      "    2 | 49m27s | \u001b[35m-1138.77875\u001b[0m | \u001b[32m            0.9174\u001b[0m | \u001b[32m   0.1188\u001b[0m | \u001b[32m     4.9612\u001b[0m | \u001b[32m          173.8000\u001b[0m | \u001b[32m     0.9352\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[861]\ttrain-mae:1071.66+1.58757\ttest-mae:1139.42+4.86947\n",
      "\n",
      "    3 | 33m23s | -1139.42093 |             0.8877 |    0.1340 |      5.6713 |           194.1738 |      0.8407 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[953]\ttrain-mae:1065.23+1.96049\ttest-mae:1139.99+5.85049\n",
      "\n",
      "    4 | 35m03s | -1139.99011 |             0.8216 |    0.1964 |      5.1108 |           183.0585 |      0.8149 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[5894]\ttrain-mae:1103.84+2.30925\ttest-mae:1142.74+5.83644\n",
      "\n",
      "    5 | 109m21s | -1142.73843 |             0.8006 |    0.1320 |      2.9602 |           197.2073 |      0.8973 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[651]\ttrain-mae:1054.83+2.06474\ttest-mae:1139.87+6.18899\n",
      "\n",
      "    6 | 33m07s | -1139.87418 |             1.0000 |    0.2000 |      6.0000 |           160.0000 |      0.8000 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[658]\ttrain-mae:1056.36+0.709278\ttest-mae:1138.91+5.64801\n",
      "\n",
      "    7 | 31m26s | -1138.91324 |             1.0000 |    0.0000 |      6.0000 |           173.8910 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4810]\ttrain-mae:1109.11+1.92417\ttest-mae:1143.96+6.3716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.53821143]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 90m06s | -1143.95685 |             0.8000 |    0.2000 |      2.0000 |           172.6061 |      0.8000 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[789]\ttrain-mae:1053.09+2.56312\ttest-mae:1140.06+7.57559\n",
      "\n",
      "    9 | 33m16s | -1140.06323 |             0.8000 |    0.2000 |      6.0000 |           200.0000 |      0.8000 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[791]\ttrain-mae:1048.05+2.45294\ttest-mae:1139.27+6.35895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.26070374]), 'nit': 3, 'funcalls': 45}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 | 36m08s | -1139.26886 |             0.8000 |    0.2000 |      6.0000 |           176.5578 |      0.8000 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[961]\ttrain-mae:1065.02+2.10662\ttest-mae:1139.12+5.55878\n",
      "\n",
      "   11 | 45m36s | -1139.12286 |             0.9857 |    0.0822 |      5.9400 |           193.0848 |      0.8005 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4778]\ttrain-mae:1106.97+1.95695\ttest-mae:1142.99+6.55656\n",
      "\n",
      "   12 | 112m44s | -1142.99216 |             0.9783 |    0.1099 |      2.6692 |           163.0974 |      0.8423 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1051]\ttrain-mae:1057.95+2.44399\ttest-mae:1138.8+5.81845\n",
      "\n",
      "   13 | 46m55s | -1138.79816 |             0.9447 |    0.0308 |      5.8746 |           186.3225 |      0.8936 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4773]\ttrain-mae:1107.46+1.71463\ttest-mae:1142.75+6.79527\n",
      "\n",
      "   14 | 97m42s | -1142.74536 |             0.9148 |    0.0478 |      2.5695 |           160.1405 |      0.8799 | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   15 | 50m51s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[933]\ttrain-mae:1045.77+4.98671\ttest-mae:1138.38+6.23862\n",
      "\n",
      "   16 | 40m25s | \u001b[35m-1138.38351\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   0.2000\u001b[0m | \u001b[32m     6.0000\u001b[0m | \u001b[32m          189.4664\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   17 | 53m18s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   18 | 50m35s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   19 | 50m22s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   20 | 50m28s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   21 | 50m20s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   22 | 50m11s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   23 | 50m19s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   24 | 50m13s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   25 | 50m20s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   26 | 50m14s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   27 | 50m17s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   28 | 50m20s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   29 | 50m09s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mae:1071.14+1.80143\ttest-mae:1139.12+6.26434\n",
      "\n",
      "   30 | 88m13s | -1139.12234 |             0.9279 |    0.1254 |      4.8055 |           190.8394 |      0.9581 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "\n",
    "    # change objective function?\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=10000, nfold=4,\n",
    "                       obj = xgb_fairobj, # change it if necessary\n",
    "                       feval=eval_mae,\n",
    "                       seed=1234, callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (max_depth - 1, max_depth + 3),\n",
    "                              'min_child_weight': (min_child_weight - 20, min_child_weight + 20),\n",
    "                              'colsample_bytree': (max(colsample_bytree - 0.2, 0.1), min(colsample_bytree + 0.2, 1)),\n",
    "                              'subsample': (max(subsample - 0.2, 0.1), min(subsample + 0.2, 1)),\n",
    "                              'gamma': (max(gamma - 0.25, 0), gamma + 0.2)\n",
    "                             }\n",
    "                            )\n",
    "xgb_BO.maximize(init_points=5, n_iter=25)\n",
    "del xgtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>189.466444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1138.383515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.874609</td>\n",
       "      <td>186.322461</td>\n",
       "      <td>0.893639</td>\n",
       "      <td>0.944709</td>\n",
       "      <td>0.030789</td>\n",
       "      <td>-1138.798157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>173.891017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1138.913239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.805533</td>\n",
       "      <td>190.839429</td>\n",
       "      <td>0.958068</td>\n",
       "      <td>0.927896</td>\n",
       "      <td>0.125444</td>\n",
       "      <td>-1139.122345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.940027</td>\n",
       "      <td>193.084757</td>\n",
       "      <td>0.800454</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.082193</td>\n",
       "      <td>-1139.122864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>176.557829</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1139.268860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1139.874176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1140.063233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.569533</td>\n",
       "      <td>160.140501</td>\n",
       "      <td>0.879932</td>\n",
       "      <td>0.914782</td>\n",
       "      <td>0.047822</td>\n",
       "      <td>-1142.745361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.669176</td>\n",
       "      <td>163.097411</td>\n",
       "      <td>0.842253</td>\n",
       "      <td>0.978335</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>-1142.992157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>172.606093</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1143.956848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  subsample  colsample_bytree     gamma  \\\n",
       "10   6.000000        189.466444   1.000000          0.800000  0.200000   \n",
       "7    5.874609        186.322461   0.893639          0.944709  0.030789   \n",
       "1    6.000000        173.891017   1.000000          1.000000  0.000000   \n",
       "12   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "14   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "23   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "22   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "21   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "20   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "19   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "18   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "17   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "16   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "15   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "24   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "13   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "11   4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "9    4.805533        190.839429   0.958068          0.927896  0.125444   \n",
       "5    5.940027        193.084757   0.800454          0.985692  0.082193   \n",
       "4    6.000000        176.557829   0.800000          0.800000  0.200000   \n",
       "0    6.000000        160.000000   0.800000          1.000000  0.200000   \n",
       "3    6.000000        200.000000   0.800000          0.800000  0.200000   \n",
       "8    2.569533        160.140501   0.879932          0.914782  0.047822   \n",
       "6    2.669176        163.097411   0.842253          0.978335  0.109853   \n",
       "2    2.000000        172.606093   0.800000          0.800000  0.200000   \n",
       "\n",
       "          score  \n",
       "10 -1138.383515  \n",
       "7  -1138.798157  \n",
       "1  -1138.913239  \n",
       "12 -1139.122345  \n",
       "14 -1139.122345  \n",
       "23 -1139.122345  \n",
       "22 -1139.122345  \n",
       "21 -1139.122345  \n",
       "20 -1139.122345  \n",
       "19 -1139.122345  \n",
       "18 -1139.122345  \n",
       "17 -1139.122345  \n",
       "16 -1139.122345  \n",
       "15 -1139.122345  \n",
       "24 -1139.122345  \n",
       "13 -1139.122345  \n",
       "11 -1139.122345  \n",
       "9  -1139.122345  \n",
       "5  -1139.122864  \n",
       "4  -1139.268860  \n",
       "0  -1139.874176  \n",
       "3  -1140.063233  \n",
       "8  -1142.745361  \n",
       "6  -1142.992157  \n",
       "2  -1143.956848  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'subsample',\n",
    "                                       'colsample_bytree',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134.8576701708903, 27419)\n",
      "(1116.6790009215167, 17972)\n",
      "(1131.1384039528332, 21360)\n",
      "(1115.2645121508447, 14962)\n",
      "(1140.7384546532385, 17910)\n",
      "(1130.7031351835888, 20260)\n",
      "(1130.2607257929228, 19997)\n",
      "(1130.3115546841811, 20444)\n",
      "(1119.252611666559, 16626)\n",
      "(1118.2582621231877, 23023)\n",
      "(-1126.7464331299761, 19997.299999999999)\n"
     ]
    }
   ],
   "source": [
    "fold = 10\n",
    "obj = logregobj\n",
    "train_pred = np.zeros((train_x.shape[0], 1))\n",
    "test_pred = np.zeros((test_x.shape[0], fold))\n",
    "    \n",
    "skf = list(KFold(len(train_y), fold))\n",
    "best_rounds = []\n",
    "scores=[]\n",
    "for i, (train, val) in enumerate(skf):\n",
    "    est = xgb.XGBRegressor(\n",
    "                       learning_rate = 0.005,\n",
    "                       n_estimators = 500000,\n",
    "                       max_depth= 7,\n",
    "                       min_child_weight=183, \n",
    "                       colsample_bytree=0.2,\n",
    "                       subsample=0.8,\n",
    "                       gamma =0.5,\n",
    "                       objective = obj,\n",
    "                       nthread = -1,\n",
    "                       silent = True\n",
    "                      )\n",
    "    est.fit(train_x[train],train_y[train],\n",
    "            eval_set=[(train_x[val], train_y[val])], \n",
    "            eval_metric = eval_mae, \n",
    "            early_stopping_rounds = 500, verbose = False)\n",
    "    val_y_predict_fold = est.predict(train_x[val])\n",
    "    train_pred[val,0] = val_y_predict_fold\n",
    "    test_pred[:,i] = est.predict(test_x)\n",
    "    score = log_mae(train_y[val], val_y_predict_fold)\n",
    "    print (score, est.best_iteration)\n",
    "    scores.append(score)\n",
    "    best_rounds.append(est.best_iteration)\n",
    "print (-np.mean(scores), np.mean(best_rounds))\n",
    "\n",
    "train_pred = np.exp(train_pred) - lift\n",
    "test_pred = np.exp(test_pred) - lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_pred, columns = ['Pred']).to_csv('../output/train_pred.csv', index = False)\n",
    "test_pred = pd.DataFrame(test_pred, columns = ['Pred_'+str(k) for k in range(fold)])\n",
    "test_pred['avgPred'] = test_pred.mean(axis = 1)\n",
    "test_pred.to_csv('../output/test_pred.csv', index = False)\n",
    "pd.DataFrame({'id': ID, 'loss': test_pred['avgPred']}).to_csv('../output/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mannual Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val: ', 0, 'MAE: ', 1145.2003352659963, 'best_round:', 2002)\n",
      "('val: ', 0.001, 'MAE: ', 1149.6040074938803, 'best_round:', 1370)\n",
      "('val: ', 0.003, 'MAE: ', 1145.1468359398334, 'best_round:', 2442)\n",
      "('val: ', 0.01, 'MAE: ', 1147.9205449647668, 'best_round:', 1549)\n",
      "('val: ', 0.03, 'MAE: ', 1146.300458525963, 'best_round:', 1696)\n",
      "('val: ', 0.1, 'MAE: ', 1146.477290412138, 'best_round:', 1975)\n",
      "('val: ', 0.3, 'MAE: ', 1148.031414890273, 'best_round:', 1536)\n",
      "('val: ', 1, 'MAE: ', 1146.9586082476646, 'best_round:', 1810)\n",
      "('val: ', 3, 'MAE: ', 1146.3344093325347, 'best_round:', 1971)\n"
     ]
    }
   ],
   "source": [
    "# tune num_leaves (255, doulbe/half) -> min_data_in_leaf (100, +/-20), feature_fraction (1, -0.1), \n",
    "# bagging_fraction (1, -0.1), bagging_freq (0, 1, 2, ...), max_bin (255, doulbe/half)\n",
    "learning_rate = 0.1\n",
    "obj = logregobj\n",
    "\n",
    "num_leaves=7\n",
    "min_child_weight=180\n",
    "colsample_bytree=0.6\n",
    "subsample=1\n",
    "subsample_freq=0\n",
    "max_bin=255\n",
    "#reg_alpha=0.003\n",
    "\n",
    "for val in [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]:\n",
    "    rgr = lgb.LGBMRegressor(seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                   learning_rate=learning_rate,\n",
    "                   n_estimators=10000,\n",
    "                   objective=obj,\n",
    "                   nthread = -1, #The acutal cores of CPU\n",
    "                   num_leaves=num_leaves,\n",
    "                   min_child_weight = min_child_weight,\n",
    "                   colsample_bytree = colsample_bytree,\n",
    "                   subsample = subsample,\n",
    "                   subsample_freq = subsample_freq,\n",
    "                   max_bin = max_bin,\n",
    "                   reg_alpha = val,\n",
    "                   silent = True)\n",
    "    rgr.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=lgbm_eval_mae, early_stopping_rounds=50, verbose = False)\n",
    "    y_pred = rgr.predict(X_val)\n",
    "    print(\"val: \", val, \"MAE: \", log_mae(y_val,y_pred), \"best_round:\", rgr.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_leaves=50\n",
    "min_child_weight=180\n",
    "colsample_bytree=0.1\n",
    "subsample=0.8\n",
    "subsample_freq=1\n",
    "max_bin=484\n",
    "reg_alpha=0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Automated tuning - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_weight |   num_leaves |   reg_alpha |   subsample | \n",
      "(1142.4069211093899, 1389)\n",
      "(1153.4811690241861, 1166)\n",
      "(1147.1241301297898, 1300)\n",
      "(1145.0921910299503, 1325)\n",
      "    1 | 24m43s | \u001b[35m-1147.02610\u001b[0m | \u001b[32m            0.0116\u001b[0m | \u001b[32m 352.9165\u001b[0m | \u001b[32m          208.9087\u001b[0m | \u001b[32m     90.6704\u001b[0m | \u001b[32m     0.0015\u001b[0m | \u001b[32m     0.8028\u001b[0m | \n",
      "(1142.5668075373803, 489)\n",
      "(1148.5171100983987, 379)\n",
      "(1144.2946569336498, 369)\n",
      "(1143.3360282357949, 470)\n",
      "    2 | 10m05s | \u001b[35m-1144.67865\u001b[0m | \u001b[32m            0.0713\u001b[0m | \u001b[32m 459.6612\u001b[0m | \u001b[32m          212.3745\u001b[0m | \u001b[32m     94.4639\u001b[0m | \u001b[32m     0.0054\u001b[0m | \u001b[32m     0.7071\u001b[0m | \n",
      "(1141.0394608107106, 608)\n",
      "(1147.1890154565699, 764)\n",
      "(1143.3356862634337, 565)\n",
      "(1139.7332630428054, 577)\n",
      "    3 | 12m16s | \u001b[35m-1142.82436\u001b[0m | \u001b[32m            0.0376\u001b[0m | \u001b[32m 389.7326\u001b[0m | \u001b[32m          203.6682\u001b[0m | \u001b[32m     70.7976\u001b[0m | \u001b[32m     0.0006\u001b[0m | \u001b[32m     0.8649\u001b[0m | \n",
      "(1145.7045654688911, 1108)\n",
      "(1152.0598946045234, 1225)\n",
      "(1147.7028887131121, 1425)\n",
      "(1147.2034157668222, 1336)\n",
      "    4 | 21m47s | -1148.16769 |             0.0107 |  424.9811 |           201.1686 |      92.1606 |      0.0079 |      0.7161 | \n",
      "(1140.7561885863429, 445)\n",
      "(1147.7838478890858, 419)\n",
      "(1143.048924486209, 396)\n",
      "(1145.0331729895349, 491)\n",
      "    5 | 10m16s | -1144.15553 |             0.0907 |  497.5039 |           183.4318 |      84.1448 |      0.0004 |      0.8329 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_weight |   num_leaves |   reg_alpha |   subsample | \n",
      "(1140.985622529606, 824)\n",
      "(1145.5818937515867, 740)\n",
      "(1145.8347887428288, 795)\n",
      "(1140.5632340239133, 747)\n",
      "    6 | 10m59s | -1143.24138 |             0.2000 |  255.0000 |           220.0000 |      31.0000 |      0.0000 |      1.0000 | \n",
      "(1140.2900193861553, 1947)\n",
      "(1145.7808390872669, 2349)\n",
      "(1146.1937252936159, 1492)\n",
      "(1143.4463969014662, 1931)\n",
      "    7 | 18m01s | -1143.92775 |             0.0100 |  511.0000 |           220.0000 |      31.0000 |      0.0000 |      1.0000 | \n",
      "(1140.8483735125988, 386)\n",
      "(1145.1829688424084, 244)\n",
      "(1146.4344403460827, 292)\n",
      "(1142.9754491073597, 316)\n",
      "    8 | 08m35s | -1143.86031 |             0.2000 |  511.0000 |           220.0000 |     127.0000 |      0.0000 |      1.0000 | \n",
      "(1142.233059031938, 781)\n",
      "(1144.1078335459797, 762)\n",
      "(1144.6783250119959, 756)\n",
      "(1142.5644554512305, 694)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.10760583]), 'nit': 3, 'funcalls': 46}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 07m25s | -1143.39592 |             0.2000 |  382.1268 |           160.0000 |      31.0000 |      0.0000 |      0.7000 | \n",
      "(1140.7759344541539, 903)\n",
      "(1144.906313506069, 748)\n",
      "(1143.9340420381127, 883)\n",
      "(1142.3140691095168, 887)\n",
      "   10 | 08m32s | -1142.98259 |             0.2000 |  390.6923 |           220.0000 |      31.0000 |      0.0000 |      1.0000 | \n",
      "(1145.95613508055, 1098)\n",
      "(1153.0461585346436, 1062)\n",
      "(1147.104359596515, 1377)\n",
      "(1147.3060495386319, 1443)\n",
      "   11 | 20m41s | -1148.35318 |             0.0100 |  255.0000 |           160.0000 |     127.0000 |      0.0000 |      1.0000 | \n",
      "(1142.9376155235273, 1611)\n",
      "(1150.9856633824013, 1502)\n",
      "(1148.7007253408169, 1465)\n",
      "(1147.2306155604042, 1707)\n",
      "   12 | 17m48s | -1147.46365 |             0.0100 |  386.8388 |           220.0000 |      60.1823 |      0.0000 |      0.7000 | \n",
      "(1146.2808096417707, 212)\n",
      "(1146.8047664250946, 248)\n",
      "(1148.1099264757588, 312)\n",
      "(1143.4470080060532, 278)\n",
      "   13 | 06m56s | -1146.16063 |             0.2000 |  511.0000 |           160.0000 |     127.0000 |      0.0000 |      1.0000 | \n",
      "(1143.6468246082447, 890)\n",
      "(1144.9486390066093, 823)\n",
      "(1148.3151500347572, 773)\n",
      "(1142.1518770465359, 742)\n",
      "   14 | 07m56s | -1144.76562 |             0.2000 |  511.0000 |           160.0000 |      31.0000 |      0.0000 |      1.0000 | \n",
      "(1142.5158299991476, 403)\n",
      "(1146.5243300506411, 361)\n",
      "(1146.1602597771064, 569)\n",
      "(1142.1306422877844, 493)\n",
      "   15 | 08m03s | -1144.33277 |             0.2000 |  386.8750 |           185.7028 |      68.3533 |      0.0000 |      1.0000 | \n",
      "(1138.6079767327967, 625)\n",
      "(1145.7015923569259, 760)\n",
      "(1142.7807184232659, 775)\n",
      "(1141.6251171020906, 645)\n",
      "   16 | 07m25s | \u001b[35m-1142.17885\u001b[0m | \u001b[32m            0.1173\u001b[0m | \u001b[32m 418.0644\u001b[0m | \u001b[32m          172.3508\u001b[0m | \u001b[32m     32.5954\u001b[0m | \u001b[32m     0.0037\u001b[0m | \u001b[32m     0.7735\u001b[0m | \n",
      "(1142.5452152831303, 1764)\n",
      "(1144.7502550128168, 2944)\n",
      "(1143.797191559313, 1869)\n",
      "(1142.3383833057492, 2094)\n",
      "   17 | 27m45s | -1143.35776 |             0.0100 |  408.3047 |           195.5919 |      31.0000 |      0.0100 |      1.0000 | \n",
      "(1139.8876398233497, 352)\n",
      "(1148.6874198793525, 244)\n",
      "(1145.3148876666869, 287)\n",
      "(1146.162925300343, 303)\n",
      "   18 | 10m30s | -1145.01322 |             0.1147 |  378.8946 |           217.0790 |     121.1688 |      0.0078 |      0.7989 | \n",
      "(1139.1996939705043, 890)\n",
      "(1144.0370281077526, 910)\n",
      "(1141.8325535348285, 1105)\n",
      "(1139.364200728741, 844)\n",
      "   19 | 12m09s | \u001b[35m-1141.10837\u001b[0m | \u001b[32m            0.0424\u001b[0m | \u001b[32m 448.1240\u001b[0m | \u001b[32m          169.6323\u001b[0m | \u001b[32m     32.8217\u001b[0m | \u001b[32m     0.0081\u001b[0m | \u001b[32m     0.7191\u001b[0m | \n",
      "(1139.0022927096747, 516)\n",
      "(1147.0222987869508, 402)\n",
      "(1142.9160858613627, 557)\n",
      "(1139.4981324385387, 575)\n",
      "   20 | 11m10s | -1142.10970 |             0.1049 |  380.6139 |           206.8872 |      64.9523 |      0.0085 |      0.9873 | \n",
      "(1135.6377422030566, 671)\n",
      "(1145.5725486408192, 801)\n",
      "(1144.4653333698516, 562)\n",
      "(1139.7739406667288, 493)\n",
      "   21 | 10m13s | -1141.36239 |             0.1584 |  368.3032 |           212.3441 |      35.9703 |      0.0000 |      0.7692 | \n",
      "(1143.3375267200195, 695)\n",
      "(1145.826081074289, 857)\n",
      "(1145.3354240505219, 791)\n",
      "(1141.510812209872, 729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.40716893]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 10m38s | -1144.00246 |             0.2000 |  367.8985 |           203.3066 |      31.0000 |      0.0100 |      1.0000 | \n",
      "(1137.2692522112668, 766)\n",
      "(1144.5144231353736, 750)\n",
      "(1141.6512614047936, 733)\n",
      "(1138.5227630583731, 775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.38710871]), 'nit': 5, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 11m22s | \u001b[35m-1140.48942\u001b[0m | \u001b[32m            0.0549\u001b[0m | \u001b[32m 424.8913\u001b[0m | \u001b[32m          160.4803\u001b[0m | \u001b[32m     41.5332\u001b[0m | \u001b[32m     0.0083\u001b[0m | \u001b[32m     0.7553\u001b[0m | \n",
      "(1139.5764197044923, 506)\n",
      "(1144.4577249000829, 509)\n",
      "(1143.1163485362224, 459)\n",
      "(1142.10659256442, 549)\n",
      "   24 | 12m43s | -1142.31427 |             0.0639 |  362.8903 |           206.2861 |      78.9789 |      0.0006 |      0.9121 | \n",
      "(1142.8938779601274, 539)\n",
      "(1146.1017059047183, 400)\n",
      "(1144.6648630011669, 320)\n",
      "(1143.3059809401154, 447)\n",
      "   25 | 10m57s | -1144.24161 |             0.0571 |  484.9354 |           206.7286 |     101.6738 |      0.0053 |      0.7767 | \n",
      "(1140.7108210887407, 434)\n",
      "(1148.4262357361306, 488)\n",
      "(1146.1394910899328, 593)\n",
      "(1142.4953920871264, 490)\n",
      "   26 | 09m37s | -1144.44299 |             0.1230 |  453.8276 |           160.4153 |      53.5665 |      0.0024 |      0.7022 | \n",
      "(1142.1579119940977, 1945)\n",
      "(1150.3592475494152, 1503)\n",
      "(1146.3569679854315, 1875)\n",
      "(1146.2343618406551, 1726)\n",
      "   27 | 21m52s | -1146.27712 |             0.0100 |  433.6820 |           160.0000 |      31.0000 |      0.0100 |      0.7000 | \n",
      "(1142.9697961457932, 677)\n",
      "(1147.4495180183869, 583)\n",
      "(1147.3173129171532, 715)\n",
      "(1143.3279868803415, 658)\n",
      "   28 | 10m21s | -1145.26615 |             0.2000 |  448.5103 |           186.3029 |      39.5613 |      0.0090 |      1.0000 | \n",
      "(1143.2712400635075, 714)\n",
      "(1148.3033061597594, 595)\n",
      "(1144.6629189823184, 556)\n",
      "(1142.0830558316716, 644)\n",
      "   29 | 13m18s | -1144.58013 |             0.0310 |  472.7721 |           183.0922 |      82.8874 |      0.0082 |      0.7862 | \n",
      "(1143.5264649582491, 1572)\n",
      "(1150.0346949482596, 1503)\n",
      "(1146.9958398810354, 1688)\n",
      "(1146.63769269973, 1729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didle/OtherSoftwares/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.17646752]), 'nit': 5, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 25m03s | -1146.79867 |             0.0100 |  416.6993 |           160.0000 |      52.3347 |      0.0000 |      0.7000 | \n",
      "(1140.4757824921196, 349)\n",
      "(1151.4633353799902, 254)\n",
      "(1146.6472026729778, 294)\n",
      "(1143.3490886231871, 359)\n",
      "   31 | 09m43s | -1145.48385 |             0.1801 |  386.1441 |           181.5562 |     101.2623 |      0.0090 |      0.8047 | \n",
      "(1139.7336118058795, 501)\n",
      "(1146.7807043108555, 639)\n",
      "(1144.6715590799881, 856)\n",
      "(1138.8146426316732, 873)\n",
      "   32 | 10m11s | -1142.50013 |             0.1222 |  433.3767 |           168.8109 |      35.3339 |      0.0080 |      0.8825 | \n",
      "(1142.8869426446361, 441)\n",
      "(1148.0578310235217, 398)\n",
      "(1145.4569996699884, 352)\n",
      "(1142.1483668258034, 410)\n",
      "   33 | 11m15s | -1144.63754 |             0.0695 |  446.7031 |           179.0185 |     108.1515 |      0.0011 |      0.7853 | \n",
      "(1141.7666671883608, 315)\n",
      "(1147.6011921974773, 344)\n",
      "(1144.947497439736, 311)\n",
      "(1143.6227701353603, 301)\n",
      "   34 | 10m29s | -1144.48453 |             0.1314 |  455.0124 |           207.8048 |     122.4469 |      0.0093 |      0.7885 | \n",
      "(1139.8093671812253, 1014)\n",
      "(1146.9612957902648, 791)\n",
      "(1143.4492325460014, 702)\n",
      "(1142.5379175224996, 747)\n",
      "   35 | 14m37s | -1143.18945 |             0.0250 |  425.8923 |           195.8064 |      60.7620 |      0.0068 |      0.7826 | \n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "obj = logregobj\n",
    "def lgbm_cv_bagging(max_bin, num_leaves, min_child_weight, colsample_bytree, subsample, reg_alpha, learning_rate=0.1):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est = lgb.LGBMRegressor(seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                   learning_rate=learning_rate,\n",
    "                   n_estimators=10000,\n",
    "                   objective=obj,\n",
    "                   nthread = -1, #The acutal cores of CPU\n",
    "                   num_leaves=int(num_leaves),\n",
    "                   min_child_weight = int(min_child_weight),\n",
    "                   colsample_bytree = colsample_bytree,\n",
    "                   subsample = subsample,\n",
    "                   subsample_freq = int(subsample_freq),\n",
    "                   max_bin = int(max_bin),\n",
    "                   reg_alpha = reg_alpha,\n",
    "                   silent = True)\n",
    "        train_x_fold = train_x[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.fit(train_x_fold, train_y_fold, eval_set=[(val_x_fold, val_y_fold)], eval_metric=lgbm_eval_mae, early_stopping_rounds=50, verbose = False)\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold)\n",
    "        print (score, est.best_iteration)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "def lgbm_cv_nobagging(max_bin, num_leaves, min_child_weight, colsample_bytree, reg_alpha, learning_rate=0.1):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est = lgb.LGBMRegressor(seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                   learning_rate=learning_rate,\n",
    "                   n_estimators=10000,\n",
    "                   objective=obj,\n",
    "                   nthread = -1, #The acutal cores of CPU\n",
    "                   num_leaves=int(num_leaves),\n",
    "                   min_child_weight = int(min_child_weight),\n",
    "                   colsample_bytree = colsample_bytree,\n",
    "                   subsample = 1,\n",
    "                   subsample_freq = 0,\n",
    "                   max_bin = int(max_bin),\n",
    "                   reg_alpha = reg_alpha,\n",
    "                   silent = True)\n",
    "        train_x_fold = train_x[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.fit(train_x_fold, train_y_fold, eval_set=[(val_x_fold, val_y_fold)], eval_metric=lgbm_eval_mae, early_stopping_rounds=50, verbose = False)\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold)\n",
    "        print (score, est.best_iteration)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv_bagging, {\n",
    "                                     'max_bin': (255, 511),\n",
    "                                     'num_leaves': (31, 127),\n",
    "                                     'min_child_weight' :(160, 220),\n",
    "                                     'colsample_bytree': (0.01, 0.2),\n",
    "                                     'subsample' : (0.7, 1),\n",
    "                                     'reg_alpha': (0, 0.01)})\n",
    "\n",
    "#lgbm_BO = BayesianOptimization(lgbm_cv_nobagging, {\n",
    "#                                     'max_bin': (255, 511),\n",
    "#                                     'num_leaves': (31, 127),\n",
    "#                                     'min_child_weight' :(140, 240),\n",
    "#                                     'colsample_bytree': (0.1, 0.8),\n",
    "#                                     'reg_alpha': (0, 0.01)})\n",
    "lgbm_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>max_bin</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>41.533235</td>\n",
       "      <td>160.480263</td>\n",
       "      <td>424.891268</td>\n",
       "      <td>0.054892</td>\n",
       "      <td>0.755303</td>\n",
       "      <td>8.349064e-03</td>\n",
       "      <td>-1140.489425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32.821725</td>\n",
       "      <td>169.632318</td>\n",
       "      <td>448.124003</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>0.719071</td>\n",
       "      <td>8.139682e-03</td>\n",
       "      <td>-1141.108369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.970286</td>\n",
       "      <td>212.344100</td>\n",
       "      <td>368.303197</td>\n",
       "      <td>0.158388</td>\n",
       "      <td>0.769226</td>\n",
       "      <td>4.163086e-05</td>\n",
       "      <td>-1141.362391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64.952260</td>\n",
       "      <td>206.887199</td>\n",
       "      <td>380.613910</td>\n",
       "      <td>0.104926</td>\n",
       "      <td>0.987334</td>\n",
       "      <td>8.513890e-03</td>\n",
       "      <td>-1142.109702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.595404</td>\n",
       "      <td>172.350784</td>\n",
       "      <td>418.064367</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>0.773481</td>\n",
       "      <td>3.703591e-03</td>\n",
       "      <td>-1142.178851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>78.978943</td>\n",
       "      <td>206.286086</td>\n",
       "      <td>362.890256</td>\n",
       "      <td>0.063890</td>\n",
       "      <td>0.912071</td>\n",
       "      <td>6.213211e-04</td>\n",
       "      <td>-1142.314271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35.333856</td>\n",
       "      <td>168.810942</td>\n",
       "      <td>433.376664</td>\n",
       "      <td>0.122177</td>\n",
       "      <td>0.882492</td>\n",
       "      <td>7.954188e-03</td>\n",
       "      <td>-1142.500129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>390.692263</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1142.982590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60.761983</td>\n",
       "      <td>195.806403</td>\n",
       "      <td>425.892291</td>\n",
       "      <td>0.025045</td>\n",
       "      <td>0.782639</td>\n",
       "      <td>6.830397e-03</td>\n",
       "      <td>-1143.189453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1143.241385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>195.591913</td>\n",
       "      <td>408.304721</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-1143.357761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>382.126793</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1143.395918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1143.860308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1143.927745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>203.306606</td>\n",
       "      <td>367.898462</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-1144.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101.673793</td>\n",
       "      <td>206.728569</td>\n",
       "      <td>484.935405</td>\n",
       "      <td>0.057132</td>\n",
       "      <td>0.776694</td>\n",
       "      <td>5.349111e-03</td>\n",
       "      <td>-1144.241607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>68.353270</td>\n",
       "      <td>185.702828</td>\n",
       "      <td>386.875007</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.813732e-11</td>\n",
       "      <td>-1144.332766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>53.566479</td>\n",
       "      <td>160.415305</td>\n",
       "      <td>453.827585</td>\n",
       "      <td>0.123036</td>\n",
       "      <td>0.702212</td>\n",
       "      <td>2.437709e-03</td>\n",
       "      <td>-1144.442985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>122.446851</td>\n",
       "      <td>207.804789</td>\n",
       "      <td>455.012370</td>\n",
       "      <td>0.131351</td>\n",
       "      <td>0.788479</td>\n",
       "      <td>9.285567e-03</td>\n",
       "      <td>-1144.484532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>82.887424</td>\n",
       "      <td>183.092162</td>\n",
       "      <td>472.772129</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>0.786198</td>\n",
       "      <td>8.165078e-03</td>\n",
       "      <td>-1144.580130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>108.151503</td>\n",
       "      <td>179.018535</td>\n",
       "      <td>446.703136</td>\n",
       "      <td>0.069454</td>\n",
       "      <td>0.785324</td>\n",
       "      <td>1.053486e-03</td>\n",
       "      <td>-1144.637535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1144.765623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>121.168842</td>\n",
       "      <td>217.079019</td>\n",
       "      <td>378.894572</td>\n",
       "      <td>0.114745</td>\n",
       "      <td>0.798930</td>\n",
       "      <td>7.754003e-03</td>\n",
       "      <td>-1145.013218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39.561326</td>\n",
       "      <td>186.302933</td>\n",
       "      <td>448.510343</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000049e-03</td>\n",
       "      <td>-1145.266153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>101.262273</td>\n",
       "      <td>181.556240</td>\n",
       "      <td>386.144096</td>\n",
       "      <td>0.180126</td>\n",
       "      <td>0.804709</td>\n",
       "      <td>9.024489e-03</td>\n",
       "      <td>-1145.483852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.232641e-08</td>\n",
       "      <td>-1146.160628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>433.681954</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-1146.277122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>52.334673</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>416.699270</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1146.798673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60.182348</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>386.838799</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1147.463655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1148.353176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_leaves  min_child_weight     max_bin  colsample_bytree  subsample  \\\n",
       "17   41.533235        160.480263  424.891268          0.054892   0.755303   \n",
       "13   32.821725        169.632318  448.124003          0.042388   0.719071   \n",
       "15   35.970286        212.344100  368.303197          0.158388   0.769226   \n",
       "14   64.952260        206.887199  380.613910          0.104926   0.987334   \n",
       "10   32.595404        172.350784  418.064367          0.117317   0.773481   \n",
       "18   78.978943        206.286086  362.890256          0.063890   0.912071   \n",
       "26   35.333856        168.810942  433.376664          0.122177   0.882492   \n",
       "4    31.000000        220.000000  390.692263          0.200000   1.000000   \n",
       "29   60.761983        195.806403  425.892291          0.025045   0.782639   \n",
       "0    31.000000        220.000000  255.000000          0.200000   1.000000   \n",
       "11   31.000000        195.591913  408.304721          0.010000   1.000000   \n",
       "3    31.000000        160.000000  382.126793          0.200000   0.700000   \n",
       "2   127.000000        220.000000  511.000000          0.200000   1.000000   \n",
       "1    31.000000        220.000000  511.000000          0.010000   1.000000   \n",
       "16   31.000000        203.306606  367.898462          0.200000   1.000000   \n",
       "19  101.673793        206.728569  484.935405          0.057132   0.776694   \n",
       "9    68.353270        185.702828  386.875007          0.200000   1.000000   \n",
       "20   53.566479        160.415305  453.827585          0.123036   0.702212   \n",
       "28  122.446851        207.804789  455.012370          0.131351   0.788479   \n",
       "23   82.887424        183.092162  472.772129          0.030973   0.786198   \n",
       "27  108.151503        179.018535  446.703136          0.069454   0.785324   \n",
       "8    31.000000        160.000000  511.000000          0.200000   1.000000   \n",
       "12  121.168842        217.079019  378.894572          0.114745   0.798930   \n",
       "22   39.561326        186.302933  448.510343          0.200000   1.000000   \n",
       "25  101.262273        181.556240  386.144096          0.180126   0.804709   \n",
       "7   127.000000        160.000000  511.000000          0.200000   1.000000   \n",
       "21   31.000000        160.000000  433.681954          0.010000   0.700000   \n",
       "24   52.334673        160.000000  416.699270          0.010000   0.700000   \n",
       "6    60.182348        220.000000  386.838799          0.010000   0.700000   \n",
       "5   127.000000        160.000000  255.000000          0.010000   1.000000   \n",
       "\n",
       "       reg_alpha        score  \n",
       "17  8.349064e-03 -1140.489425  \n",
       "13  8.139682e-03 -1141.108369  \n",
       "15  4.163086e-05 -1141.362391  \n",
       "14  8.513890e-03 -1142.109702  \n",
       "10  3.703591e-03 -1142.178851  \n",
       "18  6.213211e-04 -1142.314271  \n",
       "26  7.954188e-03 -1142.500129  \n",
       "4   0.000000e+00 -1142.982590  \n",
       "29  6.830397e-03 -1143.189453  \n",
       "0   0.000000e+00 -1143.241385  \n",
       "11  1.000000e-02 -1143.357761  \n",
       "3   0.000000e+00 -1143.395918  \n",
       "2   0.000000e+00 -1143.860308  \n",
       "1   0.000000e+00 -1143.927745  \n",
       "16  1.000000e-02 -1144.002461  \n",
       "19  5.349111e-03 -1144.241607  \n",
       "9   1.813732e-11 -1144.332766  \n",
       "20  2.437709e-03 -1144.442985  \n",
       "28  9.285567e-03 -1144.484532  \n",
       "23  8.165078e-03 -1144.580130  \n",
       "27  1.053486e-03 -1144.637535  \n",
       "8   0.000000e+00 -1144.765623  \n",
       "12  7.754003e-03 -1145.013218  \n",
       "22  9.000049e-03 -1145.266153  \n",
       "25  9.024489e-03 -1145.483852  \n",
       "7   2.232641e-08 -1146.160628  \n",
       "21  1.000000e-02 -1146.277122  \n",
       "24  0.000000e+00 -1146.798673  \n",
       "6   0.000000e+00 -1147.463655  \n",
       "5   0.000000e+00 -1148.353176  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_bo_scores = pd.DataFrame([[s[0]['num_leaves'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['max_bin'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['reg_alpha'],\n",
    "                               s[1]] for s in zip(lgbm_BO.res['all']['params'],lgbm_BO.res['all']['values'])],\n",
    "                            columns = ['num_leaves',\n",
    "                                       'min_child_weight',\n",
    "                                       'max_bin',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'reg_alpha',\n",
    "                                       'score'])\n",
    "gbm_bo_scores=gbm_bo_scores.sort_values('score',ascending=False)\n",
    "gbm_bo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation & Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1133.1663002518465, 19214)\n",
      "(1117.0386514953771, 15911)\n",
      "(1131.7961480318784, 18736)\n",
      "(1114.0050047316508, 15253)\n",
      "(1140.0568886975177, 15355)\n",
      "(1131.9771909108647, 17264)\n",
      "(1129.8376509126649, 18345)\n",
      "(1129.9701815935912, 17848)\n",
      "(1119.4439506783817, 18554)\n",
      "(1117.2647914303179, 15547)\n",
      "(-1126.4556758734091, 17202.700000000001)\n"
     ]
    }
   ],
   "source": [
    "fold = 10\n",
    "train_pred = np.zeros((train_x.shape[0], 1))\n",
    "test_pred = np.zeros((test_x.shape[0], fold))\n",
    "    \n",
    "skf = list(KFold(len(train_y), fold))\n",
    "scores=[]\n",
    "best_rounds=[]\n",
    "for i, (train, val) in enumerate(skf):\n",
    "    est=lgb.LGBMRegressor(learning_rate=0.005,\n",
    "                   n_estimators=500000,\n",
    "                   objective=obj,\n",
    "                   nthread = -1, #The acutal cores of CPU\n",
    "                   num_leaves=42,\n",
    "                   min_child_weight = 160,\n",
    "                   colsample_bytree = 0.055,\n",
    "                   subsample = 0.76,\n",
    "                   subsample_freq = 1,\n",
    "                   max_bin = 424,\n",
    "                   reg_alpha = 0.008,\n",
    "                   silent = True)\n",
    "    est.fit(train_x[train], train_y[train], eval_set=[(train_x[val], train_y[val])], \n",
    "            eval_metric=lgbm_eval_mae, early_stopping_rounds=500, verbose = False)\n",
    "    val_y_predict_fold = est.predict(train_x[val])\n",
    "    train_pred[val,0] = val_y_predict_fold\n",
    "    test_pred[:,i] = est.predict(test_x)\n",
    "    score = log_mae(train_y[val], val_y_predict_fold)\n",
    "    print (score, est.best_iteration)\n",
    "    best_rounds.append(est.best_iteration)\n",
    "    scores.append(score)\n",
    "print (-np.mean(scores), np.mean(best_rounds))\n",
    "train_pred = np.exp(train_pred) - lift\n",
    "test_pred = np.exp(test_pred) - lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_pred, columns = ['Pred']).to_csv('../output/train_pred.csv', index = False)\n",
    "test_pred = pd.DataFrame(test_pred, columns = ['Pred_'+str(k) for k in range(fold)])\n",
    "test_pred['avgPred'] = test_pred.mean(axis = 1)\n",
    "test_pred.to_csv('../output/test_pred.csv', index = False)\n",
    "pd.DataFrame({'id': ID, 'loss': test_pred['avgPred']}).to_csv('../output/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Repnum = 5\n",
    "pred_train_y = np.zeros((train_x.shape[0], Repnum))\n",
    "pred_test_y = np.zeros((test_x.shape[0], Repnum))\n",
    "for k in range(Repnum):\n",
    "    rgr = lgb.LGBMRegressor(learning_rate=0.005,                             \n",
    "                     n_estimators=17202,\n",
    "                     objective=obj,\n",
    "                     nthread = -1, #The acutal cores of CPU\n",
    "                     max_bin=424,\n",
    "                     num_leaves=42,\n",
    "                     min_child_samples=160,\n",
    "                     colsample_bytree=0.055,\n",
    "                     subsample=0.76,\n",
    "                     subsample_freq=1,\n",
    "                     reg_alpha=0.008,\n",
    "                     silent=True)\n",
    "    rgr.fit(train_x, train_y)\n",
    "    \n",
    "    pred_test_y[:, k] = np.exp(rgr.predict(test_x)) - lift\n",
    "    pd.DataFrame({'prob': pred_test_y[:,k]}).to_csv('../output/test_'+str(k)+'.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "\n",
    "## Comment out following lines if you are using Theano as backend\n",
    "#import tensorflow as tf\n",
    "#tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom metric function for Keras\n",
    "def mae_log(y_true, y_pred): \n",
    "    lift = 200\n",
    "    return K.mean(K.abs((K.exp(y_pred)-lift) - (K.exp(y_true)-lift)))\n",
    "\n",
    "# Keras deosn't support sparse matrix. \n",
    "# The following functions are useful to split a large sparse matrix into smaller batches so they can be loaded into mem.\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0 \n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='mae_log', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='mae_log', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(400, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim # fixed length: number of columns of X\n",
    "                   ))\n",
    "    \n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(200)) # number of hidden units. needs to be tuned.\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # 1 for regression \n",
    "    model.compile(loss = 'mae',\n",
    "                  metrics=[mae_log],\n",
    "                  optimizer = 'adadelta' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit = model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "early_stop = EarlyStopping(monitor='mae_log', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='mae_log', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['input_size'], input_dim = params['input_dim']))\n",
    "\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['input_drop_out']))\n",
    "        \n",
    "    model.add(Dense(params['hidden_size']))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out']))\n",
    "    \n",
    "    nadam = Nadam(lr=params['learning_rate'])\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "    return(model)\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(parameters)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(parameters)))\n",
    "    scores = np.zeros ((len(skf),len(parameters)))\n",
    "    best_rounds = np.zeros ((len(skf),len(parameters)))\n",
    " \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "        print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "            \n",
    "            # early stopping\n",
    "            model = nn_model(nn_params)\n",
    "            print (model)\n",
    "            \n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=60,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     callbacks=[\n",
    "#                                                 EarlyStopping(monitor='mae_log'\n",
    "#                                                               , patience=early_stopping_rounds, verbose=0, mode='auto'),\n",
    "                                                ModelCheckpoint(filepath=\"weights.hdf5\"\n",
    "                                                                , monitor='mae_log', \n",
    "                                                                verbose=0, save_best_only=True, mode='min')\n",
    "                                                ]\n",
    "                                     )\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "\n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_generator(generator=batch_generatorp(val_x_fold, batch_size, False),\n",
    "                                        val_samples=val_x_fold.shape[0]\n",
    "                                     )\n",
    "            \n",
    "            score = log_mae(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val, j] = val_y_predict_fold.reshape(val_y_predict_fold.shape[0])\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')            \n",
    "            test_blend_x_j[:,i] = model.predict_generator(generator=batch_generatorp(test_x, batch_size, True),\n",
    "                                        val_samples=test_x.shape[0]\n",
    "                                     ).reshape(test_x.shape[0])\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_parameters = [\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :450 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 250 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 250 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :450 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :350 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.6 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 150 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.1,\n",
    "     'learning_rate': 0.1}\n",
    "]\n",
    "\n",
    "(train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_x, train_y, test_x,\n",
    "                                                         4,\n",
    "                                                         5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x = np.exp(train_blend_x) - 200\n",
    "test_blend_x = np.exp(test_blend_x) - 200\n",
    "np.savetxt('../output/cv_train_keras10.csv', train_blend_x, delimiter=\",\")\n",
    "np.savetxt('../output/cv_test_keras10.csv', test_blend_x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = np.exp(test_blend_x[:,3:4]) - 200 # the forth column of test_blend_x\n",
    "results = pd.DataFrame()\n",
    "results['id'] = ID\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_starter.csv\", index=False)\n",
    "print (\"Submission created.\")\n",
    "\n",
    "pred_y = np.exp(np.mean(test_blend_x,axis=1)) - 200\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_mean.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finding weights by MCMC ...\n"
     ]
    }
   ],
   "source": [
    "def MAE(pred,y):\n",
    "    error = np.exp(pred) -np.exp(y)\n",
    "    error = np.mean((error**2)**.5)\n",
    "    return 'mcc error',error\n",
    "    \n",
    "# original form\n",
    "def MAE2(pred,y):\n",
    "    error = pred - y\n",
    "    error = np.mean((error**2)**.5)\n",
    "    return 'mcc error',error\n",
    "\n",
    "#### NOW THE MCMC PART to find individal weights for ensemble####\n",
    "num = train.shape[1]\n",
    "#weight = np.array([1.0/num,]*num)\n",
    "weight = np.array([ 0.02523733,  0.00312543,  0.00631371,  0.00700795,  0.00810079,\n",
    "        0.01606908,  0.01690393,  0.07645415,  0.04172416,  0.00971234,\n",
    "        0.06701633,  0.001     ,  0.02779518,  0.02927811,  0.01636343,\n",
    "        0.00781184,  0.001     ,  0.001     ,  0.08138719,  0.001     ,\n",
    "        0.01751576,  0.04822087,  0.02539103,  0.02555227,  0.0514218 ,\n",
    "        0.01235628,  0.03889011,  0.20541707,  0.06427481,  0.0818134 ])\n",
    "# This is to define variables to be used later\n",
    "Repnum = 10\n",
    "\n",
    "train_mcmc=np.zeros((train.shape[0], Repnum))\n",
    "test_mcmc_1=np.zeros((test_1.shape[0], Repnum))\n",
    "test_mcmc_2=np.zeros((test_2.shape[0], Repnum))\n",
    "for k in range(Repnum):\n",
    "    pred_new = np.zeros(train.shape[0])\n",
    "    pred_old = np.zeros(train.shape[0])\n",
    "    counter = 0\n",
    "    n=10000 ###MCMC steps\n",
    "    result={}\n",
    "    \n",
    "    print('\\n Finding weights by MCMC ...')\n",
    "    for i in range(num):\n",
    "        pred_new += train[:,i]*weight[i]\n",
    "    pred_old = pred_new\n",
    "    \n",
    "    #### MCMC  #### \n",
    "    ### MCMC algo for dummies \n",
    "    ### 1. Get initialize ensemble weights\n",
    "    ### 2. Generate new weights \n",
    "    ### 3. if MAE is lower, accept new weights immediately , or else accept new weights with probability of np.exp(-diff/.3)\n",
    "    ### 4. repeat 2-3\n",
    "    for i in range(n):\n",
    "        new_weights = weight+ np.array([0.005,]*num)*np.random.normal(loc=0.0, scale=1.0, size=num)\n",
    "        new_weights[new_weights < 0.001]=0.001 #0.01=>0.001\n",
    "        pred_new=np.zeros(train.shape[0])\n",
    "        for ii in range(num):\n",
    "            pred_new += train[:,ii]*new_weights[ii]\n",
    "        diff = MAE2(pred_new,train_y)[1] - MAE2(pred_old,train_y)[1]\n",
    "        prob = min(1,np.exp(-diff/.5)) #0.3 -> 0.5\n",
    "        random_prob = np.random.rand()\n",
    "        if random_prob < prob:\n",
    "            weight = new_weights\n",
    "            pred_old = pred_new\n",
    "            result[i] = (MAE2(pred_new,train_y)[1] ,MAE2(pred_old,train_y)[1],prob,random_prob ,weight)\n",
    "            counter +=1\n",
    "    print (counter *1.0 / n, 'Acceptance Ratio') #keep this [0.4,0.6] for best results\n",
    "    print ('best result MAE', sorted([result[i] for i in result])[0:1][0])\n",
    "        \n",
    "    weight=sorted([result[i] for i in result])[0:1][-1]\n",
    "    weight = weight[-1]\n",
    "        \n",
    "    for i in range(num):\n",
    "        train_mcmc[:,k] += train[:,i]*weight[i]\n",
    "        test_mcmc_1[:,k] += test_1[:,i]*weight[i]\n",
    "        test_mcmc_2[:,k] += test_2[:,i]*weight[i]\n",
    "    print ('combined all features plus MCMC weights:',',MAE=', MAE2(train_mcmc[:,k],train_y))\n",
    "    print ('weights:', weight)\n",
    "### notice the weights do not necessarily sum to 1 ###\n",
    "\n",
    "#train_mcmc=np.exp(train_mcmc) - lift\n",
    "#test_mcmc_1=np.exp(test_mcmc_1) - lift\n",
    "#test_mcmc_2=np.exp(test_mcmc_2) - lift\n",
    "\n",
    "train_pred = pd.DataFrame(train_mcmc, columns = ['Pred_'+str(k) for k in range(Repnum)])\n",
    "train_pred['avgPred'] = train_pred.mean(axis=1)\n",
    "train_pred['id'] = train_id['id']\n",
    "train_pred['loss'] = train_id['loss']\n",
    "train_pred.to_csv('../output/final_mcmc30K_train_97.csv', index = False)\n",
    "\n",
    "test_pred1 = pd.DataFrame(test_mcmc_1, columns = ['Pred_'+str(k) for k in range(Repnum)])\n",
    "test_pred1['avgPred'] = test_pred1.mean(axis = 1)\n",
    "test_pred1['id'] = test_id['id']\n",
    "#test_pred1.to_csv('../output/final_mcmc30K_test1_9.csv', index = False)\n",
    "test_pred1[['id','avgPred']].to_csv('../output/sub_final_mcmc30K_test1_97.csv', header = ['id','loss'], index = False)\n",
    "\n",
    "test_pred2 = pd.DataFrame(test_mcmc_2, columns = ['Pred_'+str(k) for k in range(Repnum)])\n",
    "test_pred2['avgPred'] = test_pred2.mean(axis = 1)\n",
    "test_pred2['id'] = test_id['id']\n",
    "#test_pred2.to_csv('../output/final_mcmc30K_test2_9.csv', index = False)\n",
    "test_pred2[['id','avgPred']].to_csv('../output/sub_final_mcmc30K_test2_97.csv', header = ['id','loss'], index = False)\n",
    "\n",
    "test_id['loss'] = 0.5*test_pred1['avgPred']+0.5*test_pred2['avgPred']\n",
    "test_id.to_csv('../output/final_mcmc30K_test_97.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
