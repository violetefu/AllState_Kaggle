{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allstate week 3\n",
    "\n",
    "This week we will learn how to:\n",
    "\n",
    "* tune LightGBM\n",
    "* create Neural Networks with Keras (Theano or Tensorflow backend)\n",
    "* tune Neural Networks\n",
    "* create a simple ensemble of XGBoost, LightGBM and Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy import sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logregobj(labels, preds):\n",
    "    con = 2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "def log_mae(labels,preds,lift=200):\n",
    "    return mean_absolute_error(np.exp(labels)-lift, np.exp(preds)-lift)\n",
    "\n",
    "log_mae_scorer = metrics.make_scorer(log_mae, greater_is_better = False)\n",
    "\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = log_mae_scorer,\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
    "\n",
    "def xgb_logregobj(preds, dtrain):\n",
    "    con = 2\n",
    "    labels = dtrain.get_label()\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def search_model_mae (train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'neg_mean_absolute_error',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "start = time.time() \n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))        \n",
    "\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge train and test\n",
    "\n",
    "This will save our time on duplicating logics for train and test and will also ensure the transformations applied on train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data\n",
    "                       ,test_data])\n",
    "del( train_data, test_data)\n",
    "print (\"Full Data set created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group features\n",
    "\n",
    "In this step we will group the features into different groups so we can preprocess them seperately afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')\n",
    "\n",
    "print (\"Categorical features:\", cat_cols)\n",
    "print ( \"Numerica features:\", num_cols)\n",
    "print ( \"ID: %s, target: %s\" %( id_col, target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features \n",
    "### 1. Label Encoding (Factorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "start=time.time()\n",
    "for cat_col in cat_cols:\n",
    "#     print (\"Factorize feature %s\" % (cat))\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "print ('Label enconding finished in %f seconds' % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One Hot Encoding (get dummies)\n",
    "\n",
    "OHE can be done by either Pandas' get_dummies() or SK Learn's OneHotEncoder. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings, but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* We will convert the OHE's results to a sparse matrix which uses way less memory as compared to dense matrix. However, not all algorithms and packagers support sparse matrix, e.g. Keras. In that case, we'll need to use other tricks to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "print (full_data_sparse.shape)\n",
    "\n",
    "## it should be (313864, 1176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave-one-out Encoding\n",
    "\n",
    "This is a very useful trick that has been used by many Kaggle winning solutions. It's particularly effective for high cardinality categorical features, postal code for instance. However, it doesn't seem to help a lot for this competition and the following code is just FYI. Feel free to skip it as it may take long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# loo_cols =[]\n",
    "# for col in cat_cols:\n",
    "#     print (\"Leave-One-Out Encoding  %s\" % (col))\n",
    "#     print (\"Leave-one-out encoding column %s for %s......\" % (col, target_col))\n",
    "#     aggr=full_data.groupby(col)[target_col].agg([np.mean]).join(full_data[:train_size].groupby(col)[target_col].agg([np.sum,np.size]),how='left')        \n",
    "#     meanTagetAggr = np.mean(aggr['mean'].values)\n",
    "#     aggr=full_data.join(aggr,how='left', on=col)[list(aggr.columns)+[target_col]]\n",
    "#     loo_col = 'MEAN_BY_'+col+'_'+target_col\n",
    "#     full_data[loo_col] = \\\n",
    "#     aggr.apply(lambda row: row['mean'] if math.isnan(row[target_col]) \n",
    "#                                                        else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(0.95, 1.05) , axis=1)\n",
    "#     loo_cols.append(loo_col)\n",
    "#     print (\"New feature %s created.\" % (loo_col))\n",
    "# print ('Leave-one-out enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "We will apply two preprocessings on numeric features:\n",
    "\n",
    "1. Apply box-cox transformations for skewed numeric features.\n",
    "\n",
    "2. Scale numeric features so they will fall in the range between 0 and 1.\n",
    "\n",
    "Please be advised that these preprocessings are not necessary for tree-based models, e.g. XGBoost. However, linear or linear-based models, which will be dicussed in following weeks, may benefit from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculate skewness of each numeric features: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, boxcox\n",
    "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
    "print (skewed_cols.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply box-cox transformations: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply Standard Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SSL = preprocessing.StandardScaler()\n",
    "# for num_col in num_cols:\n",
    "#     full_data[num_col] = SSL.fit_transform(full_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: LBL and OHE are likely exclusive so we will use one of them at a time combined with numeric features. In the following steps we will use OHE + Numeric to tune XGBoost models and you can apply the same process with OHE + Numeric features. Averaging results from two different models will likely generate better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse\n",
    "                                  ,full_data[num_cols])\n",
    "                                 , format='csr'\n",
    "                                 )\n",
    "print (full_data_sparse.shape)\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Tuning\n",
    "\n",
    "* LightGBM\n",
    "\n",
    "https://github.com/Microsoft/LightGBM\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "    * Faster training speed and higher efficiency\n",
    "    * Lower memory usage\n",
    "    * Better accuracy\n",
    "    * Parallel learning supported\n",
    "    * Capable of handling large-scale data\n",
    "\n",
    "* pyLIghtGBM\n",
    "\n",
    "pyLightGBM is a python binding for Microsoft LightGBM\n",
    "\n",
    "https://github.com/ArdalanM/pyLightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tune num_leaves\n",
    "* default=127, type=int, alias=num_leaf\n",
    "* number of leaves in one tree\n",
    "* control overfit\n",
    "    * Smaller: unerfit\n",
    "    * larger: overfit\n",
    "* start from default (127), double of half the number and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tune num_leaves\n",
    "from pylightgbm.models import GBMRegressor\n",
    "\n",
    "rgr = GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                   learning_rate=0.1,\n",
    "                   metric = 'l1',\n",
    "                   num_threads = 4, #The acutal cores of CPU\n",
    "                   num_iterations=10000,\n",
    "                   early_stopping_round=50,\n",
    "                   num_leaves=127,\n",
    "                   verbose = True)\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        test_data=[(X_val,y_val)])\n",
    "\n",
    "y_pred = rgr.predict(X_val)\n",
    "print rgr.best_round\n",
    "print(\"MAE: \", log_mae(y_val,y_pred, 200))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_leaves = <best num_leaves>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tune min_data_in_leaf\n",
    "* default=100, type=int, alias=min_data_per_leaf , min_data\n",
    "* Minimal number of data in one leaf. Can use this to deal with over-fit.\n",
    "* control overfit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit\n",
    "* start from default (100), increase or descrese by 20 and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tune min_data_in_leaf\n",
    "\n",
    "rgr = GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                   learning_rate=0.1,\n",
    "                   metric = 'l1',\n",
    "                   num_threads = 4, #The acutal cores of CPU\n",
    "                   num_iterations=10000,\n",
    "                   early_stopping_round=50,\n",
    "                   num_leaves=num_leaves,\n",
    "                   min_data_in_leaf=100,\n",
    "                   verbose = True)\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        test_data=[(X_val,y_val)])\n",
    "\n",
    "y_pred = rgr.predict(X_val)\n",
    "print rgr.best_round\n",
    "print(\"MAE: \", log_mae(y_val,y_pred, 200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_data_in_leaf = <best min_data_in_leaf>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tune feature_fraction\n",
    "* feature_fraction, default=1.0, type=double, 0.0 < feature_fraction < 1.0, alias=sub_feature\n",
    "* LightGBM will random select part of features on each iteration if feature_fraction smaller than 1.0. For example, if * set to 0.8, will select 80% features before training each tree.\n",
    "* Can use this to speed up training\n",
    "* Can use this to deal with over-fit\n",
    "    * Smaller: overfit\n",
    "    * larger: underfit\n",
    "* start from default (1), descrese by 0.1 and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tune feature_fraction\n",
    "\n",
    "rgr = GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                   learning_rate=0.1,\n",
    "                   metric = 'l1',\n",
    "                   num_threads = 4, #The acutal cores of CPU\n",
    "                   num_iterations=num_iterations,\n",
    "                   early_stopping_round=early_stopping_round,\n",
    "                   num_leaves=num_leaves,\n",
    "                   min_data_in_leaf=min_data_in_leaf,\n",
    "                   feature_fraction = 1,\n",
    "\n",
    "                   verbose = True)\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        test_data=[(X_val,y_val)])\n",
    "\n",
    "y_pred = rgr.predict(X_val)\n",
    "print rgr.best_round\n",
    "print(\"MAE: \", log_mae(y_val,y_pred, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_fraction = <best feature_fraction>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tune bagging_freq\n",
    "* default=0, type=int\n",
    "* Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "* Note: To enable bagging, should set bagging_fraction as well (1 is recommended).\n",
    "* start from default (1), descrese by 0.1 and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tune bagging_fraction\n",
    "rgr = GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                   learning_rate=0.1,\n",
    "                   metric = 'l1',\n",
    "                   num_threads = 4, #The acutal cores of CPU\n",
    "                   num_iterations=num_iterations,\n",
    "                   early_stopping_round=early_stopping_round,\n",
    "                   num_leaves=num_leaves,\n",
    "                   min_data_in_leaf=min_data_in_leaf,\n",
    "                   feature_fraction = feature_fraction,\n",
    "                   bagging_freq = 1, # this has to be set to an integer greater than 0 to enable bagging\n",
    "                   bagging_fraction = 1,\n",
    "                   verbose = True)\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        test_data=[(X_val,y_val)])\n",
    "\n",
    "y_pred = rgr.predict(X_val)\n",
    "print rgr.best_round\n",
    "print(\"MAE: \", log_mae(y_val,y_pred, 200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging_fraction = <best bagging_fraction>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tune bagging_freq\n",
    "* default=0, type=int\n",
    "* Frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration.\n",
    "* Note: To enable bagging, should set bagging_fraction as well (1 is recommended).\n",
    "* start from default (1), descrese by 0.1 and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tune max_bin\n",
    "rgr = GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                   learning_rate=0.1,\n",
    "                   metric = 'l1',\n",
    "                   num_threads = 4, #The acutal cores of CPU\n",
    "                   num_iterations=10000,\n",
    "                   early_stopping_round=50,\n",
    "                   num_leaves=num_leaves,\n",
    "                   min_data_in_leaf=min_data_in_leaf,\n",
    "                   feature_fraction = feature_fraction,\n",
    "                   bagging_freq = 1,\n",
    "                   bagging_fraction = bagging_fraction,\n",
    "                   max_bin = 255,\n",
    "                   verbose = True)\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        test_data=[(X_val,y_val)])\n",
    "\n",
    "y_pred = rgr.predict(X_val)\n",
    "print rgr.best_round\n",
    "print(\"MAE: \", log_mae(y_val,y_pred, 200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tune max_bin\n",
    "* default=255, type=int\n",
    "* max number of bin that feature values will bucket in. Small bin may reduce training accuracy but may increase general power (deal with over-fit).\n",
    "* start from default (255), double of half the number and check if it improves. repeat the process until there's no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_bin = <best max_bin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning - Bayesian Optimization\n",
    "\n",
    "Github: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "The idea is to set a range for each parameters, for which we can leverage the parameters from manual tuning, then let the bayersian optimization to seek best parameters.\n",
    "\n",
    "It's more efficient than grid search but is still time consuming. Therefore knowing an approximate range of values for each parameter will greatly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from pylightgbm.models import GBMRegressor\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "\n",
    "\n",
    "def lgbm_cv(max_bin, num_leaves, min_data_in_leaf, feature_fraction,bagging_fraction, learning_rate=0.1):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est=GBMRegressor(learning_rate = learning_rate,\n",
    "                        max_bin=int(max_bin),\n",
    "                        num_leaves=int(num_leaves),\n",
    "                        min_data_in_leaf=int(min_data_in_leaf),\n",
    "                        feature_fraction=feature_fraction,\n",
    "                        bagging_fraction=bagging_fraction,\n",
    "                        bagging_freq = 1,\n",
    "                        num_threads=4,\n",
    "                        exec_path =\"/users/cchen1/library/LightGBM/lightgbm\")\n",
    "        train_x_fold = train_x[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( num_iterations=100000)\n",
    "        est.set_params( early_stopping_round=50)\n",
    "        est.set_params( metric='l1')\n",
    "        est.set_params(verbose = False)\n",
    "        print (est)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                test_data=[(val_x_fold, val_y_fold)]\n",
    "               )\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "        print (score, est.best_round)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "            \n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv, {\n",
    "                                     'max_bin': (, ),\n",
    "                                     'num_leaves': (,),\n",
    "                                     'min_data_in_leaf' :(,),\n",
    "                                     'feature_fraction': (,),\n",
    "                                     'bagging_fraction' : (,)})\n",
    "lgbm_BO.maximize(init_points=5, n_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm_bo_scores = pd.DataFrame([[s[0]['num_leaves'],\n",
    "                               s[0]['min_data_in_leaf'],\n",
    "                               s[0]['max_bin'],\n",
    "                               s[0]['feature_fraction'],\n",
    "                               s[0]['bagging_fraction'],\n",
    "                               s[1]] for s in zip(lgbm_BO.res['all']['params'],lgbm_BO.res['all']['values'])],\n",
    "                            columns = ['num_leaves',\n",
    "                                       'min_data_in_leaf',\n",
    "                                       'max_bin',\n",
    "                                       'feature_fraction',\n",
    "                                       'bagging_fraction',\n",
    "                                       'score'])\n",
    "gbm_bo_scores=gbm_bo_scores.sort_values('score',ascending=False)\n",
    "gbm_bo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation using parameters from Bayesian Optimization\n",
    "\n",
    "* This step is to show how the tuned model performs with smaller learning rate (0.01 or smaller). You'd expect to see more iterations for LightGBM to converage. Therefore, you may want to use a larger number (200 for instance) for early stopping.\n",
    "\n",
    "* It will also provide optimized iterations (n_rounds/n_estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_data_in_leaf, feature_fraction,bagging_fraction\n",
    "            , learning_rate=0.1,early_stopping_round=50):\n",
    "    skf = list(KFold(len(train_y), 4))\n",
    "    scores=[]\n",
    "    best_rounds=[]\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        est=GBMRegressor(learning_rate = learning_rate,\n",
    "                        max_bin=int(max_bin),\n",
    "                        num_leaves=int(num_leaves),\n",
    "                        min_data_in_leaf=int(min_data_in_leaf),\n",
    "                        feature_fraction=feature_fraction,\n",
    "                        bagging_fraction=bagging_fraction,\n",
    "                        bagging_freq = 1,\n",
    "                        num_threads=4,\n",
    "                        exec_path =\"/users/cchen1/library/LightGBM/lightgbm\")\n",
    "        train_x_fold = train_x[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_x[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( num_iterations=100000)\n",
    "        est.set_params( early_stopping_round=early_stopping_round)\n",
    "        est.set_params( metric='l1')\n",
    "        est.set_params(verbose = False)\n",
    "        print (est)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                test_data=[(val_x_fold, val_y_fold)]\n",
    "               )\n",
    "        val_y_predict_fold = est.predict(val_x_fold)\n",
    "        score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "        print (score, est.best_round)\n",
    "        best_rounds.append(est.best_round)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores), np.mean(best_rounds)\n",
    "\n",
    "gbm_score, gbm_best_round = lgbm_cv(max_bin=582, \n",
    "                         num_leaves=140, \n",
    "                         min_data_in_leaf=75, \n",
    "                         feature_fraction=0.2,\n",
    "                         bagging_fraction=1, \n",
    "                         learning_rate=0.01,\n",
    "                         early_stopping_round=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission - LightGBM OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GBMRegressor(learning_rate = ,\n",
    "                        max_bin=,\n",
    "                        num_leaves=,\n",
    "                        min_data_in_leaf=,\n",
    "                        feature_fraction=,\n",
    "                        bagging_fraction=,\n",
    "                        bagging_freq = 1,\n",
    "                        num_threads=4,\n",
    "                        exec_path =\"/users/cchen1/library/LightGBM/lightgbm\")\n",
    "rgr.fit(train_x, train_y)\n",
    "\n",
    "pred_y = np.exp(rgr.predict(test_x)) - lift\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_gbm_ohe_tuned.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kearas\n",
    "\n",
    "https://keras.io\n",
    "\n",
    "Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "\n",
    "## Comment out following lines if you are using Theano as backend\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom metric function for Keras\n",
    "\n",
    "def mae_log(y_true, y_pred): \n",
    "    return K.mean(K.abs((K.exp(y_pred)-200) - (K.exp(y_true)-200)))\n",
    "\n",
    "\n",
    "# Keras deosn't support sparse matrix. \n",
    "# The following functions are useful to split a large sparse matrix into smaller batches so they can be loaded into mem.\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras starter\n",
    "\n",
    "Below is a quick starter example for creating a neural networks model using Keras. It covers the following aspects:\n",
    "1. multiple layers: 1 input, 1 hidden and 1 output\n",
    "2. normalization.\n",
    "3. dropout regularization.\n",
    "4. early stopping\n",
    "5. activate function\n",
    "6. optimizer\n",
    "6. batch training\n",
    "\n",
    "Advanced optimizers, activations and dropout regularization are the key characteristics that differentiate modern Neural Networks from conventional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_mae_log', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0, mode='auto')\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(400, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim # fixed length: number of columns of X\n",
    "                   ))\n",
    "    \n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(200)) # number of hidden units. needs to be tuned.\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # 1 for regression \n",
    "    model.compile(loss = 'mae',\n",
    "                  metrics=[mae_log],\n",
    "                  optimizer = 'adadelta' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and ...\n",
    "\n",
    "The following sample shows how to do cross validation for Keras with early stopping and much more. NN is time consuming, not to mention cross validation. In fact we can leverage every minutes we spent on training NN and make good use of them.\n",
    "\n",
    "we'll first create the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_mae_log', patience=5, verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='val_mae_log', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['input_size'], input_dim = params['input_dim']))\n",
    "\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['input_drop_out']))\n",
    "        \n",
    "    model.add(Dense(params['hidden_size']))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out']))\n",
    "    \n",
    "    \n",
    "#     nadam = Nadam(lr=1e-4)\n",
    "    nadam = Nadam(lr=params['learning_rate'])\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(parameters)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(parameters)))\n",
    "    scores = np.zeros ((len(skf),len(parameters)))\n",
    "    best_rounds = np.zeros ((len(skf),len(parameters)))\n",
    " \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "        print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "\n",
    "            # early stopping\n",
    "            model = nn_model(nn_params)\n",
    "            print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=60,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     callbacks=[\n",
    "#                                                 EarlyStopping(monitor='val_mae_log'\n",
    "#                                                               , patience=early_stopping_rounds, verbose=0, mode='auto'),\n",
    "                                                ModelCheckpoint(filepath=\"weights.hdf5\"\n",
    "                                                                , monitor='val_mae_log', \n",
    "                                                                verbose=1, save_best_only=True, mode='min')\n",
    "                                                ]\n",
    "                                     )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "\n",
    "         \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_generator(generator=batch_generatorp(val_x_fold, batch_size, True),\n",
    "                                        val_samples=val_x_fold.shape[0]\n",
    "                                     )\n",
    "            \n",
    "            score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "            print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val, j] = val_y_predict_fold.reshape(val_y_predict_fold.shape[0])\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')            \n",
    "            test_blend_x_j[:,i] = model.predict_generator(generator=batch_generatorp(test_x, batch_size, True),\n",
    "                                        val_samples=test_x.shape[0]\n",
    "                                     ).reshape(test_x.shape[0])\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a list of parameters that we thought might be working for NN, and cross validate each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_parameters = [\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :450 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 250 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1}\n",
    "\n",
    "]\n",
    "\n",
    "(train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_x, train_y, test_x,\n",
    "                                                         4,\n",
    "                                                         5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two submissions: \n",
    "\n",
    "* one is from the best CV score, the fourth in my case\n",
    "* another is the average of all four\n",
    "\n",
    "You can submit both and see if averaging helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = np.exp(test_blend_x[:,3:4]) - 200 # the forth column of test_blend_x\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_starter.csv\", index=False)\n",
    "print (\"Submission created.\")\n",
    "\n",
    "pred_y = np.exp(np.mean(test_blend_x,axis=1)) - 200\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_mean.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow up questions\n",
    "* So far we've already create five models/ submissions:\n",
    "    * XGBoost with LE\n",
    "    * XGBoost with OHE\n",
    "    * LightGBM with LE\n",
    "    * LightGBM with OHE\n",
    "    * Keras\n",
    "    \n",
    "  Now let's create another submission, or more, by avaraging them or with whatever weights working for you. It should yield better results.\n",
    "  \n",
    "    \n",
    "* Is there a way to ensemble the models even more effectively? "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
