{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allstate week 4\n",
    "\n",
    "This week we will be working on a few final steps to put everything together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy import sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_mae(labels,preds,lift=200):\n",
    "    return mean_absolute_error(np.exp(labels)-lift, np.exp(preds)-lift)\n",
    "\n",
    "def logregobj(labels, preds):\n",
    "    con = 2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "\n",
    "log_mae_scorer = metrics.make_scorer(log_mae, greater_is_better = False)\n",
    "\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = log_mae_scorer,\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
    "\n",
    "def xgb_logregobj(preds, dtrain):\n",
    "    con = 2\n",
    "    labels = dtrain.get_label()\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def search_model_mae (train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'neg_mean_absolute_error',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost blending function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(estimators), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(estimators)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(estimators)))\n",
    "    scores = np.zeros ((len(skf),len(estimators)))\n",
    "    best_rounds = np.zeros ((len(skf),len(estimators)))\n",
    "    \n",
    "\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "            if early_stopping_rounds==0: # without early stopping\n",
    "                est.fit(train_x_fold, train_y_fold)\n",
    "                best_rounds[i,j]=est.n_estimators\n",
    "                val_y_predict_fold = est.predict(val_x_fold)\n",
    "                score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "                print (\"Score: \", score)\n",
    "                scores[i,j]=score\n",
    "                train_blend_x[val, j] = val_y_predict_fold\n",
    "                test_blend_x_j[:,i] = est.predict(test_x)\n",
    "                print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            else:                        # early stopping\n",
    "                est.set_params( n_estimators=10000)\n",
    "                est.fit(train_x_fold,\n",
    "                        train_y_fold,\n",
    "                        eval_set=[(val_x_fold, val_y_fold)],\n",
    "                        eval_metric=xg_eval_mae,\n",
    "                        early_stopping_rounds=early_stopping_rounds,\n",
    "                        verbose=False\n",
    "                       )\n",
    "                best_round=est.best_iteration\n",
    "                best_rounds[i,j]=best_round\n",
    "                print (\"best round %d\" % (best_round))\n",
    "                val_y_predict_fold = est.predict(val_x_fold,ntree_limit=best_round)\n",
    "                score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "                print (\"Score: \", score)\n",
    "                scores[i,j]=score\n",
    "                train_blend_x[val, j] = val_y_predict_fold\n",
    "                test_blend_x_j[:,i] = est.predict(test_x,ntree_limit=best_round)\n",
    "                print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM blending function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from pylightgbm.models import GBMRegressor\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "\n",
    "def gbm_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(estimators), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(estimators)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(estimators)))\n",
    "    scores = np.zeros ((len(skf),len(estimators)))\n",
    "    best_rounds = np.zeros ((len(skf),len(estimators)))\n",
    " \n",
    "    for j, gbm_est in enumerate(estimators):\n",
    "        print (\"Model %d: %s\" %(j+1, gbm_est))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        params=gbm_est.get_params()\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            est=GBMRegressor()\n",
    "            est.param=params\n",
    "            est.exec_path='/users/cchen1/library/LightGBM/lightgbm'\n",
    "            print (est)\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "            if early_stopping_rounds==0: # without early stopping\n",
    "                est.fit(train_x_fold, train_y_fold)\n",
    "                best_rounds[i,j]=est.num_iterations\n",
    "                val_y_predict_fold = est.predict(val_x_fold)\n",
    "                score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "                print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "                scores[i,j]=score\n",
    "                train_blend_x[val, j] = val_y_predict_fold\n",
    "                test_blend_x_j[:,i] = est.predict(test_x)\n",
    "                print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            else:                        # early stopping\n",
    "                est.set_params( num_iterations=1000000)\n",
    "                est.set_params( early_stopping_round=early_stopping_rounds)\n",
    "                est.set_params(verbose = False)\n",
    "                est.fit(train_x_fold,\n",
    "                        train_y_fold,\n",
    "                        test_data=[(val_x_fold, val_y_fold)]\n",
    "                       )\n",
    "                best_round=est.best_round\n",
    "                best_rounds[i,j]=best_round\n",
    "                print (\"best round %d\" % (best_round))\n",
    "                val_y_predict_fold = est.predict(val_x_fold)\n",
    "                score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "                print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "                scores[i,j]=score\n",
    "                train_blend_x[val, j] = val_y_predict_fold\n",
    "                test_blend_x_j[:,i] = est.predict(test_x)\n",
    "                print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "start = time.time() \n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))        \n",
    "\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge train and test\n",
    "\n",
    "This will save our time on duplicating logics for train and test and will also ensure the transformations applied on train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data\n",
    "                       ,test_data])\n",
    "del( train_data, test_data)\n",
    "print (\"Full Data set created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group features\n",
    "\n",
    "In this step we will group the features into different groups so we can preprocess them seperately afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')\n",
    "\n",
    "print (\"Categorical features:\", cat_cols)\n",
    "print ( \"Numerica features:\", num_cols)\n",
    "print ( \"ID: %s, target: %s\" %( id_col, target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features \n",
    "### 1. Label Encoding (Factorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "start=time.time()\n",
    "for cat_col in cat_cols:\n",
    "#     print (\"Factorize feature %s\" % (cat))\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "print ('Label enconding finished in %f seconds' % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One Hot Encoding (get dummies)\n",
    "\n",
    "OHE can be done by either Pandas' get_dummies() or SK Learn's OneHotEncoder. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings, but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* We will convert the OHE's results to a sparse matrix which uses way less memory as compared to dense matrix. However, not all algorithms and packagers support sparse matrix, e.g. Keras. In that case, we'll need to use other tricks to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "print (full_data_sparse.shape)\n",
    "\n",
    "## it should be (313864, 1176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave-one-out Encoding\n",
    "\n",
    "This is a very useful trick that has been used by many Kaggle winning solutions. It's particularly effective for high cardinality categorical features, postal code for instance. However, it doesn't seem to help a lot for this competition and the following code is just FYI. Feel free to skip it as it may take long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# loo_cols =[]\n",
    "# for col in cat_cols:\n",
    "#     print (\"Leave-One-Out Encoding  %s\" % (col))\n",
    "#     print (\"Leave-one-out encoding column %s for %s......\" % (col, target_col))\n",
    "#     aggr=full_data.groupby(col)[target_col].agg([np.mean]).join(full_data[:train_size].groupby(col)[target_col].agg([np.sum,np.size]),how='left')        \n",
    "#     meanTagetAggr = np.mean(aggr['mean'].values)\n",
    "#     aggr=full_data.join(aggr,how='left', on=col)[list(aggr.columns)+[target_col]]\n",
    "#     loo_col = 'MEAN_BY_'+col+'_'+target_col\n",
    "#     full_data[loo_col] = \\\n",
    "#     aggr.apply(lambda row: row['mean'] if math.isnan(row[target_col]) \n",
    "#                                                        else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(0.95, 1.05) , axis=1)\n",
    "#     loo_cols.append(loo_col)\n",
    "#     print (\"New feature %s created.\" % (loo_col))\n",
    "# print ('Leave-one-out enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "We will apply two preprocessings on numeric features:\n",
    "\n",
    "1. Apply box-cox transformations for skewed numeric features.\n",
    "\n",
    "2. Scale numeric features so they will fall in the range between 0 and 1.\n",
    "\n",
    "Please be advised that these preprocessings are not necessary for tree-based models, e.g. XGBoost. However, linear or linear-based models, which will be dicussed in following weeks, may benefit from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculate skewness of each numeric features: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, boxcox\n",
    "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
    "print (skewed_cols.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply box-cox transformations: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply Standard Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSL = preprocessing.StandardScaler()\n",
    "for num_col in num_cols:\n",
    "    full_data[num_col] = SSL.fit_transform(full_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: LBL and OHE are likely exclusive so we will use one of them at a time combined with numeric features. In the following steps we will use OHE + Numeric to tune XGBoost models and you can apply the same process with OHE + Numeric features. Averaging results from two different models will likely generate better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Numberic features + Label Encoded Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numberic features + Label-encoded categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "lift = 200\n",
    "\n",
    "full_cols = num_cols + cat_cols\n",
    "train_x = full_data[full_cols][:train_size].values\n",
    "test_x = full_data[full_cols][train_size:].values\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your best 4 models. Feel free to add more as long as their performance are close enough to the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True)\n",
    "        ]\n",
    "\n",
    "(train_blend_x_gbm_le,\n",
    " test_blend_x_gbm_le,\n",
    " blend_scores_gbm_le,\n",
    " best_rounds_gbm_le) = gbm_blend(estimators, train_x, train_y, test_x,\n",
    "                                 4,\n",
    "                                 500) #as the learning rate decreases the number of stopping rounds need to be increased\n",
    "\n",
    "print (np.mean(blend_scores_gbm_le,axis=0))\n",
    "print (np.mean(best_rounds_gbm_le,axis=0))\n",
    "np.savetxt(\"../input/train_blend_x_gbm_le.csv\",train_blend_x_gbm_le, delimiter=\",\")\n",
    "np.savetxt(\"../input/test_blend_x_gbm_le.csv\",test_blend_x_gbm_le, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LE + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             )\n",
    "              \n",
    "              ]\n",
    "\n",
    "(train_blend_x_xgb_le,\n",
    " test_blend_x_xgb_le,\n",
    " blend_scores_xgb_le,\n",
    " best_rounds_xgb_le) = xgb_blend(estimators, \n",
    "                                      train_x, \n",
    "                                      train_y, \n",
    "                                      test_x,\n",
    "                                      4,\n",
    "                                      500)\n",
    "\n",
    "print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "print (np.mean(best_rounds_xgb_le,axis=0))\n",
    "np.savetxt(\"../input/train_blend_x_xgb_le.csv\",train_blend_x_xgb_le, delimiter=\",\")\n",
    "np.savetxt(\"../input/test_blend_x_xgb_le.csv\",test_blend_x_xgb_le, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numberic features + One-hot-encoded categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse\n",
    "                                  ,full_data[num_cols])\n",
    "                                 , format='csr'\n",
    "                                 )\n",
    "print (full_data_sparse.shape)\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True),\n",
    "              GBMRegressor(exec_path=\"/users/cchen1/library/LightGBM/lightgbm\",\n",
    "                     learning_rate=0.01, ## use smaller learning rate for better accuracies\n",
    "                     num_iterations=100000,\n",
    "                     max_bin=<>,\n",
    "                     num_leaves=<>,\n",
    "                     min_data_in_leaf=<>,\n",
    "                     feature_fraction=<>,\n",
    "                     bagging_fraction=<>,\n",
    "                     bagging_freq=1,      \n",
    "                     verbose = True)\n",
    "        ]\n",
    "\n",
    "\n",
    "(train_blend_x_gbm_ohe,\n",
    " test_blend_x_gbm_ohe,\n",
    " blend_scores_gbm_ohe,\n",
    " best_rounds_gbm_ohe) = gbm_blend(estimators, train_x, train_y, test_x,\n",
    "                                 4,\n",
    "                                 500)\n",
    "\n",
    "print (np.mean(blend_scores_gbm_ohe,axis=0))\n",
    "print (np.mean(best_rounds_gbm_ohe,axis=0))\n",
    "np.savetxt(\"../input/train_blend_x_gbm_ohe.csv\",train_blend_x_gbm_ohe, delimiter=\",\")\n",
    "np.savetxt(\"../input/test_blend_x_gbm_ohe.csv\",test_blend_x_gbm_ohe, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE +XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             ),\n",
    "              xgb.XGBRegressor(objective=logregobj,\n",
    "                              learning_rate=0.01, \n",
    "                              n_estimators=10000,\n",
    "                              max_depth=<>,\n",
    "                              min_child_weight=<>,\n",
    "                              colsample_bytree=<>,\n",
    "                              subsample=<>,\n",
    "                              gamma=1.0,\n",
    "                              nthread=-1,\n",
    "                              silent=True,\n",
    "                              seed=1234\n",
    "                             )\n",
    "              \n",
    "              ]\n",
    "\n",
    "(train_blend_x_xgb_ohe,\n",
    " test_blend_x_xgb_ohe,\n",
    " blend_scores_xgb_ohe,\n",
    " best_rounds_xgb_ohe) = xgb_blend(estimators, \n",
    "                                      train_x, \n",
    "                                      train_y, \n",
    "                                      test_x,\n",
    "                                      4,\n",
    "                                      1000)\n",
    "\n",
    "print (np.mean(blend_scores_xgb_ohe,axis=0))\n",
    "print (np.mean(best_rounds_xgb_ohe,axis=0))\n",
    "np.savetxt(\"../input/train_blend_x_xgb_ohe.csv\",train_blend_x_xgb_ohe, delimiter=\",\")\n",
    "np.savetxt(\"../input/test_blend_x_xgb_ohe.csv\",test_blend_x_xgb_ohe, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "\n",
    "##comment out the following two lines if you are using theano\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "\n",
    "def mae_log(y_true, y_pred):\n",
    "    return K.mean(K.abs((K.exp(y_pred)-200) - (K.exp(y_true)-200)))\n",
    "\n",
    "\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP blend function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_mae_log', patience=5, verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"../tmp/weights.hdf5\", monitor='val_mae_log', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['input_size'], input_dim = params['input_dim']))\n",
    "\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['input_drop_out']))\n",
    "        \n",
    "    model.add(Dense(params['hidden_size']))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out']))\n",
    "    \n",
    "    \n",
    "#     nadam = Nadam(lr=1e-4)\n",
    "    nadam = Nadam(lr=params['learning_rate'])\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mae', metrics=[mae_log], optimizer = params['optimizer'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(parameters)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(parameters)))\n",
    "    scores = np.zeros ((len(skf),len(parameters)))\n",
    "    best_rounds = np.zeros ((len(skf),len(parameters)))\n",
    " \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "        print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "\n",
    "            # early stopping\n",
    "            model = nn_model(nn_params)\n",
    "            print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=70,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     verbose = 0,\n",
    "                                     callbacks=[\n",
    "#                                                 EarlyStopping(monitor='val_mae_log'\n",
    "#                                                               , patience=early_stopping_rounds, verbose=0, mode='auto'),\n",
    "                                                ModelCheckpoint(filepath=\"../tmp/weights.hdf5\"\n",
    "                                                                , monitor='val_mae_log', \n",
    "                                                                verbose=1, save_best_only=True, mode='min')\n",
    "                                                ]\n",
    "                                     )\n",
    "\n",
    "            best_round=sorted([[id,mae] for [id,mae] in enumerate(fit.history['val_mae_log'])], key = lambda x:x[1], reverse = False)[0][0] \n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"../tmp/weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = nn_params['optimizer'])\n",
    "\n",
    "         \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_generator(generator=batch_generatorp(val_x_fold, batch_size, True),\n",
    "                                        val_samples=val_x_fold.shape[0]\n",
    "                                     )\n",
    "            \n",
    "            score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "            print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val, j] = val_y_predict_fold.reshape(val_y_predict_fold.shape[0])\n",
    "            \n",
    "            model.load_weights(\"../tmp/weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = nn_params['optimizer'])            \n",
    "            test_blend_x_j[:,i] = model.predict_generator(generator=batch_generatorp(test_x, batch_size, True),\n",
    "                                        val_samples=test_x.shape[0]\n",
    "                                     ).reshape(test_x.shape[0])\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagging_num = 10\n",
    "nn_parameters = []\n",
    "\n",
    "nn_parameter =  { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.3,\n",
    "     'learning_rate': 0.1,\n",
    "     'optimizer': 'adadelta'\n",
    "    }\n",
    "\n",
    "for i in range(bagging_num):\n",
    "    nn_parameters.append(nn_parameter)\n",
    "\n",
    "\n",
    "\n",
    "(train_blend_x_ohe_mlp,\n",
    " test_blend_x_ohe_mlp,\n",
    " blend_scores_ohe_mlp,\n",
    " best_round_ohe_mlp) = nn_blend_data(nn_parameters,\n",
    "                                     train_x,\n",
    "                                     train_y,\n",
    "                                     test_x,\n",
    "                                     4,\n",
    "                                     5)\n",
    "\n",
    "print (np.mean(blend_scores_ohe_mlp,axis=0))\n",
    "print (np.mean(best_round_ohe_mlp,axis=0))\n",
    "print ( log_mae(np.mean(train_blend_x_ohe_mlp,axis=1).reshape(train_size,1),train_y))\n",
    "np.savetxt(\"../input/train_blend_x_ohe_mlp.csv\",train_blend_x_ohe_mlp, delimiter=\",\")\n",
    "np.savetxt(\"../input/test_blend_x_ohe_mlp.csv\",test_blend_x_ohe_mlp, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending\n",
    "1. Ridge Regression\n",
    "  * Ridge is focused on finding out weight of each feature which is exactly what we are interested in.\n",
    "2. XGB linear\n",
    "\n",
    "Specifically, we will simply average predictions from MLP models before using them for blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ridge\n",
    "from sklearn.linear_model import ElasticNet,Ridge,LinearRegression\n",
    "print  (\"Blending.\")\n",
    "param_grid = {\n",
    "    'alpha':[0,0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,15,20,25,30,35,40,45,50,55,60,70]\n",
    "              }\n",
    "model = search_model(np.hstack((train_blend_x_gbm_le,\n",
    "                                train_blend_x_xgb_le,\n",
    "                                train_blend_x_xgb_ohe,\n",
    "                                train_blend_x_gbm_ohe,\n",
    "                                np.mean(train_blend_x_ohe_mlp,axis=1).reshape(train_size,1)))\n",
    "                                         , train_y\n",
    "                                         , Ridge()\n",
    "                                         , param_grid\n",
    "                                         , n_jobs=1\n",
    "                                         , cv=4\n",
    "                                         , refit=True)   \n",
    "\n",
    "print (\"best subsample:\", model.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x_ohe_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBoost gblinear\n",
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'booster': 'gblinear',\n",
    "    'lambda': 0,\n",
    "    'alpha': 0, # you can try different values for alpha\n",
    "    'lambda_bias' : 0,\n",
    "    'silent': 0,\n",
    "    'verbose_eval': True,\n",
    "    'seed': 1234\n",
    "}\n",
    "\n",
    "xgb.cv(params,\n",
    "       xgb.DMatrix(np.hstack((train_blend_x_gbm_le,\n",
    "                                train_blend_x_xgb_le,\n",
    "                                train_blend_x_xgb_ohe,\n",
    "                                train_blend_x_gbm_ohe,\n",
    "                                np.mean(train_blend_x_ohe_mlp,axis=1).reshape(train_size,1)))\n",
    "                   , label=train_y,missing=np.nan),\n",
    "       num_boost_round=100000, nfold=4\n",
    "                       , feval=xg_eval_mae,\n",
    "             seed=1234,\n",
    "             callbacks=[xgb.callback.early_stop(500)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y_ridge = np.exp(model.predict(np.hstack((test_blend_x_gbm_le,\n",
    "                                test_blend_x_xgb_le,\n",
    "                                test_blend_x_xgb_ohe,\n",
    "                                test_blend_x_gbm_ohe,\n",
    "                                np.mean(train_blend_x_ohe_mlp,axis=1).reshape(test_x.shape[0],1))))) - lift\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y_ridge\n",
    "results.to_csv(\"../output/sub_ridge_blended.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'booster': 'gblinear',\n",
    "    'lambda': 0,\n",
    "    'alpha': 0, # you can try different values for alpha\n",
    "    'lambda_bias' : 0,\n",
    "    'silent': 0,\n",
    "    'verbose_eval': True,\n",
    "    'seed': 1234\n",
    "}\n",
    "\n",
    "xgtrain_blend = xgb.DMatrix(np.hstack((train_blend_x_gbm_le,\n",
    "                                train_blend_x_xgb_le,\n",
    "                                train_blend_x_xgb_ohe,\n",
    "                                train_blend_x_gbm_ohe,\n",
    "                                np.mean(train_blend_x_ohe_mlp,axis=1).reshape(train_size,1))),\n",
    "                        label=train_y,missing=np.nan)\n",
    "\n",
    "xgb_model=xgb.train(params, xgtrain_blend,\n",
    "          num_boost_round=<best round of xgb.cv from above>,\n",
    "          feval=xg_eval_mae)\n",
    "\n",
    "pred_y_gblinear = np.exp(xgb_model.predict(\n",
    "        xgb.DMatrix(\n",
    "            np.hstack((test_blend_x_gbm_le,\n",
    "                       test_blend_x_xgb_le,\n",
    "                       test_blend_x_xgb_ohe,\n",
    "                       test_blend_x_gbm_ohe,\n",
    "                       np.mean(test_blend_x_ohe_mlp,axis=1).reshape(test_x.shape[0],1)))\n",
    "        )\n",
    "    )\n",
    "               ) - lift\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y_gblinear\n",
    "results.to_csv(\"../output/sub_ridge_gblinear.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission \n",
    "  weights: [0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = pred_y_ridge*0.5 + pred_y_gblinear*0.5\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_final.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional feature engineerings that you may want to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Firstly, let's take a look at importance of each original feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "full_cols = num_cols + cat_cols\n",
    "train_x = full_data[full_cols][:train_size].values\n",
    "test_x = full_data[full_cols][train_size:].values\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "rgr = xgb.XGBRegressor( seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                       learning_rate = 0.1,\n",
    "                       n_estimators = 10000,\n",
    "                       max_depth=11, #use a large max depth in hope of capturing deeper interactions\n",
    "                       colsample_bytree=1, #include all features\n",
    "                       nthread = -1,\n",
    "                       silent = True\n",
    "                      )\n",
    "\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val,\n",
    "                   y_val)],\n",
    "        eval_metric=xg_eval_mae,\n",
    "        early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "xgb_feature_importance = rgr.booster().get_fscore()\n",
    "xgb_feature_importance = [  [col,xgb_feature_importance.get('f'+str(idx),0)] for (idx,col) in enumerate(full_cols)]\n",
    "xgb_feature_importance.sort(key= lambda x:x[1], reverse=True)\n",
    "xgb_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create interactions between most important numeric features and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggr_cols = []\n",
    "aggr = full_data.groupby('cat100')['cont14'].mean().reset_index()\n",
    "aggr.columns= ['cat100', 'cont14_by_cat100']\n",
    "full_data = pd.merge(full_data,aggr, how='left',on='cat100')\n",
    "aggr_cols.append('cont14_by_cat100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train XGBoost with the new feature and check if that helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lift = 200\n",
    "\n",
    "full_cols = num_cols + cat_cols\n",
    "full_cols.append('cont14_by_cat100')\n",
    "train_x = full_data[full_cols][:train_size].values\n",
    "test_x = full_data[full_cols][train_size:].values\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)\n",
    "\n",
    "print (X_train.shape)\n",
    "rgr = xgb.XGBRegressor( seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "                       learning_rate = 0.1,\n",
    "                       n_estimators = 10000,\n",
    "                       max_depth=11, #use a large max depth in hope of capturing deeper interactions\n",
    "                       colsample_bytree=1, #include all features\n",
    "                       nthread = -1,\n",
    "                       silent = True\n",
    "                      )\n",
    "\n",
    "\n",
    "rgr.fit(X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val,\n",
    "                   y_val)],\n",
    "        eval_metric=xg_eval_mae,\n",
    "        early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "xgb_feature_importance = rgr.booster().get_fscore()\n",
    "xgb_feature_importance = [  [col,xgb_feature_importance.get('f'+str(idx),0)] for (idx,col) in enumerate(full_cols)]\n",
    "xgb_feature_importance.sort(key= lambda x:x[1], reverse=True)\n",
    "xgb_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please note that there's not guarantee that all interations will be usefull. Make sure you validate each of them and the combinations before including them in your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
